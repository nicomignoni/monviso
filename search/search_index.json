{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"monviso documentation","text":""},{"location":"#installation","title":"Installation","text":"<p>Install <code>monviso</code> directly from its GitHub repository using <code>pip</code>:</p> <pre><code>pip install git+https://github.com/nicomignoni/monviso.git@master\n</code></pre>"},{"location":"#getting-started","title":"Getting started","text":"<p>If you're already familiar with variational inequalities (VI), hop to the Quickstart for an overview on how to use <code>monviso</code>. Otherwise, the following provides an (extremely) essential introduction to VIs and to the nomenclature that will be used in the rest of the documentation. </p> <p>Given a vector mapping \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) and a scalar convex (possibly non-smooth) function \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\), solving a VI consists of solving the following </p> \\[\\begin{equation}     \\label{eq:vi_base}     \\text{find } \\mathbf{x}^* \\in \\mathbb{R}^n \\text{ such that } (\\mathbf{x} - \\mathbf{x}^*)^\\top \\mathbf{F}(\\mathbf{x}^*) - g(\\mathbf{x}) - g(\\mathbf{x}^*) \\geq 0, \\quad \\forall \\mathbf{x} \\in \\mathbb{R}^n. \\end{equation}\\] <p>It turns out that a lot of problems in optimal control, optimization, machine learning, game theory, finance, and much more, boil down to solving some instance of \\(\\eqref{eq:vi_base}\\). Such a problem is usually solved through iterative methods: one constructs an algorithm that produces a \\(\\mathbf{x}_k\\) at each iteration \\(k\\) such that \\(\\mathbf{x}_k \\to \\mathbf{x}^*\\) when \\(k \\to \\infty\\). </p> <p>What <code>monviso</code> does is providing a convenient way for accessing and using these iterative methods for solving an instance of \\(\\eqref{eq:vi_base}\\). The iterative methods that are currently implemented are listed in the API documentation. Since many of these algorithms rely on evaluating a proximal operator, <code>monviso</code> builds on top of <code>cvxpy</code>, a package for modelling and solving convex optimization problems.  </p>"},{"location":"#cite-as","title":"Cite as","text":"<p>If you used <code>monviso</code> in your project or research, please consider citing the related paper:</p> <pre><code>@inproceedings{mignoni2025monviso,\n  title={monviso: A Python Package for Solving Monotone Variational Inequalities},\n  author={Mignoni, Nicola and Baghbadorani, Reza Rahimi and Carli, Raffaele and Esfahani, Peyman Mohajerin and Dotoli, Mariagrazia and Grammatico, Sergio},\n  booktitle={2025 European Control Conference (ECC)},\n  year={2025}\n  organization={IEEE}\n}\n</code></pre>"},{"location":"#why-monviso","title":"Why \"<code>monviso</code>\"?","text":"<p>It stands for <code>mon</code>otone <code>v</code>ariational <code>i</code>nequalities <code>so</code>lutions. Initially, <code>so</code>- stood for \"solver\", but it was a bit on the nose. After all, <code>monviso</code> is just a collection of functions. Monviso is also a mountain in Italy. After a couple of iterations, the name in its current form was suggested by Sergio Grammatico.    </p>"},{"location":"api/","title":"API","text":""},{"location":"api/#the-vi-class","title":"The <code>VI</code> Class","text":""},{"location":"api/#monviso.VI","title":"monviso.VI","text":"<p>Attributes:</p> <ul> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>The size of the vector space</p> </li> <li> <code>F</code>               (<code>callable</code>)           \u2013            <p>The VI vector mapping, i.e., \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\); a function transforming a ndarray into an other <code>np.ndarray</code> of the same size.</p> </li> <li> <code>g</code>               (<code>(callable, optional)</code>)           \u2013            <p>The VI scalar mapping, i.e., \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\); a callable returning a <code>cvxpy.Expression</code></p> </li> <li> <code>S</code>               (<code>list of callable, optional</code>)           \u2013            <p>The constraints set, i.e., \\(\\mathcal{S} \\subseteq \\mathbb{R}^n\\); a list of callables, each returning a <code>Constraints</code></p> </li> </ul>"},{"location":"api/#constrained-proximal-operator","title":"Constrained proximal operator","text":""},{"location":"api/#monviso.VI.prox","title":"monviso.VI.prox","text":"<pre><code>prox(x: ndarray, **cvxpy_solve_params) -&gt; np.ndarray | None\n</code></pre> <p>Given a scalar function \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) and a constraints set \\(\\mathcal{S} \\subseteq \\mathbb{R}^n\\), the constrained proximal operator is defined as</p> \\[ \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}) = \\underset{\\mathbf{y} \\in \\mathcal{S}}{\\text{argmin}} \\left\\{ g(\\mathbf{y}) + \\frac{1}{2}\\|\\mathbf{y} - \\mathbf{x}\\|^2 \\right\\} \\] <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The proximal operator argument point</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the <code>cvxpy.Problem.solve</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The proximal operator resulting point</p> </li> </ul>"},{"location":"api/#projection-operator","title":"Projection operator","text":""},{"location":"api/#monviso.VI.proj","title":"monviso.VI.proj","text":"<pre><code>proj(x: ndarray, **cvxpy_solve_params) -&gt; np.ndarray | None\n</code></pre> <p>The projection operator of a point \\(\\mathbf{x} \\in \\mathbb{R}^n\\) with respect to set \\(\\mathcal{S} \\subseteq \\mathbb{R}^n\\) returns the closest point to \\(\\mathbf{x}\\)  that belongs to \\(\\mathcal{S}\\), i.e.,</p> \\[ \\text{proj}_{\\mathcal{S}}(\\mathbf{x}) = \\text{prox}_{0,\\mathcal{S}} (\\mathbf{x}) = \\underset{\\mathbf{y} \\in \\mathcal{S}}{\\text{argmin}} \\left\\{\\frac{1}{2}\\|\\mathbf{y} - \\mathbf{x}\\|^2 \\right\\} \\] <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The projection operator argument point</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the <code>cvxpy.Problem.solve</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The projected point</p> </li> </ul>"},{"location":"api/#iterative-methods","title":"Iterative methods","text":""},{"location":"api/#proximal-gradient","title":"Proximal gradient","text":""},{"location":"api/#monviso.VI.pg","title":"monviso.VI.pg","text":"<pre><code>pg(\n    x: ndarray, step_size: float, **cvxpy_solve_params\n) -&gt; np.ndarray | None\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and an initial vector \\(\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate of the proximal gradient (PG) algorithm is <sup>1</sup>:</p> \\[ \\mathbf{x}_{k+1} = \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k - \\chi \\mathbf{F}(\\mathbf{x}_k)) \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping.  Convergence of PG is guaranteed for Lipschitz strongly monotone operators, with monotone constant \\(\\mu &gt; 0\\) and Lipschitz constants \\(L &lt; +\\infty\\), when \\(\\chi \\in (0, 2\\mu/L^2)\\).</p> <ol> <li> <p>Nemirovskij, A. S., &amp; Yudin, D. B. (1983). Problem complexity    and method efficiency in optimization.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The steps size value, corresponding to \\(\\chi\\)</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the <code>cvxpy.Problem.solve</code> method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#extragradient","title":"Extragradient","text":""},{"location":"api/#monviso.VI.eg","title":"monviso.VI.eg","text":"<pre><code>eg(\n    x: ndarray, step_size: float, **cvxpy_solve_params\n) -&gt; np.ndarray | None\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and an initial vector  \\(\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the \\(k\\)-th iterate  of the extragradient algorithm (EG) is<sup>1</sup>:</p> \\[  \\begin{align}     \\mathbf{y}_k &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k -          \\chi \\mathbf{F}(\\mathbf{x}_k)) \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{y}_k -          \\chi \\mathbf{F}(\\mathbf{x}_k)) \\end{align} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex  (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to  \\mathbb{R}^n\\) is the VI mapping. The convergence of the EGD algorithm  is guaranteed for Lipschitz monotone operators, with Lipschitz constant  \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{L}\\right)\\).</p> <ol> <li> <p>Korpelevich, G. M. (1976). The extragradient method for finding     saddle points and other problems. Matecon, 12, 747-756.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size value, corresponding to \\(\\chi\\)</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the  <code>cvxpy.Problem.solve</code>  method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#popovs-method","title":"Popov's method","text":""},{"location":"api/#monviso.VI.popov","title":"monviso.VI.popov","text":"<pre><code>popov(\n    x: ndarray,\n    y: ndarray,\n    step_size: float,\n    **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and an initial vectors \\(\\mathbf{x}_0,\\mathbf{y}_0 \\in \\mathbb{R}^n\\), the \\(k\\)-th iterate of Popov's Method (PM) is<sup>1</sup>:</p> \\[  \\begin{align}     \\mathbf{y}_ {k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k - \\chi \\mathbf{F}(\\mathbf{y}_k)) \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{y}_{k+1} - \\chi \\mathbf{F}(\\mathbf{x}_k)) \\end{align} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping.  The convergence of PM is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{2L}\\right)\\).</p> <ol> <li> <p>Popov, L.D. A modification of the Arrow-Hurwicz method for search of saddle points.  Mathematical Notes of the Academy of Sciences of the USSR 28, 845\u2013848 (1980)\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>The initial auxiliary point, corresponding to \\(\\mathbf{y}_0\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size value, corresponding to \\(\\chi\\)</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the <code>cvxpy.Problem.solve</code> method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#forward-backward-forward","title":"Forward-backward-forward","text":""},{"location":"api/#monviso.VI.fbf","title":"monviso.VI.fbf","text":"<pre><code>fbf(\n    x: ndarray, step_size: float, **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and an initial vector \\(\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the \\(k\\)-th iterate of Forward-Backward-Forward (FBF) algorithm is<sup>1</sup>:</p> \\[  \\begin{align}     \\mathbf{y}_k &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k -          \\chi \\mathbf{F}(\\mathbf{x}_k)) \\\\     \\mathbf{x}_{k+1} &amp;= \\mathbf{y}_k -          \\chi \\mathbf{F}(\\mathbf{y}_k) + \\chi \\mathbf{F}(\\mathbf{x}_k) \\end{align} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex  (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to  \\mathbb{R}^n\\) is the VI mapping. The convergence of the FBF algorithm  is guaranteed for Lipschitz monotone operators, with Lipschitz constant  \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{L}\\right)\\).</p> <ol> <li> <p>Tseng, P. (2000). A modified forward-backward splitting method     for maximal monotone mappings. SIAM Journal on Control and     Optimization, 38(2), 431-446.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size value, corresponding to \\(\\chi\\)</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the <code>cvxpy.Problem.solve</code> method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#forward-reflected-backward","title":"Forward-reflected-backward","text":""},{"location":"api/#monviso.VI.frb","title":"monviso.VI.frb","text":"<pre><code>frb(\n    x_current: ndarray,\n    x_previous: ndarray,\n    step_size: float,\n    **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors \\(\\mathbf{x}_1,\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate of the Forward-Reflected-Backward (FRB) is the following<sup>1</sup>:</p> \\[ \\mathbf{x}_k = \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k - 2\\chi \\mathbf{F}(\\mathbf{x}_k) + \\chi \\mathbf{F}(\\mathbf{x}_{k-1})) \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of the FRB algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{2L}\\right)\\).</p> <ol> <li> <p>Malitsky, Y., &amp; Tam, M. K. (2020). A forward-backward splitting    method for monotone inclusions without cocoercivity. SIAM Journal on    Optimization, 30(2), 1451-1472.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x_current</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>x_previous</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_1\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size value, corresponding to \\(\\chi\\)</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the <code>cvxpy.Problem.solve</code> method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#projected-reflected-gradient","title":"Projected reflected gradient","text":""},{"location":"api/#monviso.VI.prg","title":"monviso.VI.prg","text":"<pre><code>prg(\n    x_current: ndarray,\n    x_previous: ndarray,\n    step_size: float,\n    **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors \\(\\mathbf{x}_1,\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate of the projected reflected gradient (PRG) is the following <sup>1</sup>:</p> \\[ \\mathbf{x}_{k+1} = \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k - \\chi \\mathbf{F}(2\\mathbf{x}_k - \\mathbf{x}_{k-1})) \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of PRG algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constants \\(L &lt; +\\infty\\), when \\(\\chi \\in (0,(\\sqrt{2} - 1)/L)\\). Differently from the EGD iteration, the PRGD has the advantage of requiring a single proximal operator evaluation.</p> <ol> <li> <p>Malitsky, Y. (2015). Projected reflected gradient methods for    monotone variational inequalities. SIAM Journal on Optimization,    25(1), 502-520.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x_current</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>x_previous</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_1\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size value, corresponding to \\(\\chi\\)</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the <code>cvxpy.Problem.solve</code> method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#extra-anchored-gradient","title":"Extra anchored gradient","text":""},{"location":"api/#monviso.VI.eag","title":"monviso.VI.eag","text":"<pre><code>eag(\n    x: ndarray, step_size: float, **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and an initial vector  \\(\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the \\(k\\)-th  iterate of extra anchored gradient (EAG) algorithm is <sup>1</sup>:</p> \\[ \\begin{align}     \\mathbf{y}_k &amp;= \\text{prox}_{g,\\mathcal{S}}\\left(\\mathbf{x}_k -          \\chi \\mathbf{F}(\\mathbf{x}_k) + \\frac{1}{k+1}(\\mathbf{x}_0 -          \\mathbf{x}_k)\\right) \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}\\left(\\mathbf{x}_k -          \\chi \\mathbf{F}(\\mathbf{y}_k) + \\frac{1}{k+1}(\\mathbf{x}_0 -          \\mathbf{x}_k)\\right) \\end{align} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex  (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to  \\mathbb{R}^n\\) is the VI mapping. The convergence of the EAG algorithm  is guaranteed for Lipschitz monotone operators, with Lipschitz constant  \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{\\sqrt{3}L} \\right)\\).</p> <ol> <li> <p>Yoon, T., &amp; Ryu, E. K. (2021, July). Accelerated Algorithms for     Smooth Convex-Concave Minimax Problems with O (1/k^ 2) Rate on Squared     Gradient Norm. In International Conference on Machine Learning (pp.     12098-12109). PMLR.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size value, corresponding to \\(\\chi\\)</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the  <code>cvxpy.Problem.solve</code>  method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#accelerated-reflected-gradient","title":"Accelerated reflected gradient","text":""},{"location":"api/#monviso.VI.arg","title":"monviso.VI.arg","text":"<pre><code>arg(\n    x_current: ndarray,\n    x_previous: ndarray,\n    step_size: float,\n    **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors  \\(\\mathbf{x}_1,\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the basic  \\(k\\)-th iterate of the accelerated reflected gradient (ARG)  is the following <sup>1</sup>:</p> \\[ \\begin{align}     \\mathbf{y}_k &amp;= 2\\mathbf{x}_k - \\mathbf{x}_{k-1} + \\frac{1}{k+1}     (\\mathbf{x}_0 - \\mathbf{x}_k) - \\frac{1}{k}(\\mathbf{x}_k -      \\mathbf{x}_{k-1}) \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}\\left(\\mathbf{x}_k -          \\chi \\mathbf{F}(\\mathbf{y}_k) + \\frac{1}{k+1}(\\mathbf{x}_0 -          \\mathbf{x}_k)\\right) \\end{align} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex  (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to  \\mathbb{R}^n\\) is the VI mapping. The convergence of the ARG algorithm  is guaranteed for Lipschitz monotone operators, with Lipschitz constant  \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{12L}\\right)\\).</p> <ol> <li> <p>Cai, Y., &amp; Zheng, W. (2022). Accelerated single-call methods     for constrained min-max optimization. arXiv preprint arXiv:2210.03096.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x_current</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>x_previous</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_1\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size value, corresponding to \\(\\chi\\)</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the  <code>cvxpy.Problem.solve</code>  method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#explicit-fast-optimistic-gradient-descent-ascent","title":"(Explicit) fast optimistic gradient descent-ascent","text":""},{"location":"api/#monviso.VI.fogda","title":"monviso.VI.fogda","text":"<pre><code>fogda(\n    x_current: ndarray,\n    x_previous: ndarray,\n    y: ndarray,\n    step_size: float,\n    alpha: float = 2.1,\n) -&gt; np.ndarray\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors  \\(\\mathbf{x}_1,\\mathbf{x}_0,\\mathbf{y}_0 \\in \\mathbb{R}^n\\), the  basic \\(k\\)-th iterate of the explicit fast OGDA (FOGDA)  is the following <sup>1</sup>:</p> \\[ \\begin{align}     \\mathbf{y}_k &amp;= \\mathbf{x}_k + \\frac{k}{k+\\alpha}(\\mathbf{x}_k -          \\mathbf{x}_{k-1}) - \\chi \\frac{\\alpha}{k+\\alpha}         \\mathbf{F}(\\mathbf{y}_{k-1}) \\\\     \\mathbf{x}_{k+1} &amp;= \\mathbf{y}_k - \\chi \\frac{2k+\\alpha}         {k+\\alpha} (\\mathbf{F}(\\mathbf{y}_k) - \\mathbf{F}(\\mathbf{y}_{k-1})) \\end{align} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex  (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to  \\mathbb{R}^n\\) is the VI mapping. The convergence of the ARG algorithm  is guaranteed for Lipschitz monotone operators, with Lipschitz constant  \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{4L}\\right)\\) and \\(\\alpha &gt; 2\\).</p> <ol> <li> <p>Bo\u0163, R. I., Csetnek, E. R., &amp; Nguyen, D. K. (2023). Fast     Optimistic Gradient Descent Ascent (OGDA) method in continuous and     discrete time. Foundations of Computational Mathematics, 1-60.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x_current</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\).</p> </li> <li> <code>x_previous</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_1\\).</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>The initial auxiliary point, corresponding to \\(\\mathbf{y}_0\\).</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size value, corresponding to \\(\\chi\\).</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>2.1</code> )           \u2013            <p>The auxiliary parameter, corresponding to the \\(\\alpha\\) parameter.</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the  <code>cvxpy.Problem.solve</code>  method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#constrained-fast-optimistic-gradient-descent-ascent","title":"Constrained fast optimistic gradient descent-ascent","text":""},{"location":"api/#monviso.VI.cfogda","title":"monviso.VI.cfogda","text":"<pre><code>cfogda(\n    x_current: ndarray,\n    x_previous: ndarray,\n    y: ndarray,\n    z: ndarray,\n    step_size: float,\n    alpha: float = 2.1,\n    **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors \\(\\mathbf{x}_1 \\in \\mathcal{S}\\), \\(\\mathbf{z}_1 \\in N_{\\mathcal{S}}(\\mathbf{x}_1)\\), \\(\\mathbf{x}_0,\\mathbf{y}_0 \\in  \\mathbb{R}^n\\), the basic \\(k\\)-th iterate of Constrained Fast Optimistic Gradient Descent Ascent (CFOGDA) is the following<sup>1</sup>:</p> \\[  \\begin{align}     \\mathbf{y}_k &amp;= \\mathbf{x}_k + \\frac{k}{k+\\alpha}(\\mathbf{x}_k -         \\mathbf{x}_{k-1}) - \\chi \\frac{\\alpha}{k+\\alpha}(         \\mathbf{F}(\\mathbf{y}_k) + \\mathbf{z}_k) \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}\\left(\\mathbf{y}_k         - \\chi\\left(1 + \\frac{k}{k+\\alpha}\\right)(\\mathbf{F}(\\mathbf{y}_k)         - \\mathbf{F}(\\mathbf{y}_{k-1}) - \\zeta_k)\\right) \\\\     \\mathbf{z}_{k+1} &amp;= \\frac{k+\\alpha}{\\chi (2k+\\alpha)}(         \\mathbf{y}_k - \\mathbf{x}_{k+1}) - (\\mathbf{F}(\\mathbf{y}_k)         - \\mathbf{F}(\\mathbf{y}_{k-1}) - \\zeta_k) \\end{align} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping.  The convergence of the CFOGDA algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{4L}\\right)\\) and \\(\\alpha &gt; 2\\).</p> <ol> <li> <p>Sedlmayer, M., Nguyen, D. K., &amp; Bot, R. I. (2023, July). A fast     optimistic method for monotone variational inequalities. In     International Conference on Machine Learning (pp. 30406-30438). PMLR.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x_current</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\).</p> </li> <li> <code>x_previous</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_1\\).</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>The initial auxiliary point, corresponding to \\(\\mathbf{y}_0\\).</p> </li> <li> <code>z</code>               (<code>ndarray</code>)           \u2013            <p>The initial auxiliary point, corresponding to \\(\\mathbf{z}_1\\).</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size value, corresponding to \\(\\chi\\).</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>2.1</code> )           \u2013            <p>The auxiliary parameter, corresponding to the \\(\\alpha\\) parameter.</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the  <code>cvxpy.Problem.solve</code>  method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#golden-ratio-algorithm","title":"Golden ratio algorithm","text":""},{"location":"api/#monviso.VI.graal","title":"monviso.VI.graal","text":"<pre><code>graal(\n    x: ndarray,\n    y: ndarray,\n    step_size: float,\n    phi: float = GOLDEN_RATIO,\n    **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors \\(\\mathbf{x}_0,\\mathbf{y}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate the golden ratio algorithm (GRAAL) is the  following <sup>1</sup>:</p> \\[  \\begin{align*}     \\mathbf{y}_{k+1} &amp;= \\frac{(\\phi - 1)\\mathbf{x}_k + \\phi\\mathbf{y}_k}     {\\phi} \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{y}_{k+1} - \\chi          \\mathbf{F}(\\mathbf{x}_k)) \\end{align*} \\] <p>The convergence of GRAAL algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constants \\(L &lt; +\\infty\\),  when \\(\\chi \\in \\left(0,\\frac{\\varphi}{2L}\\right]\\) and \\(\\phi \\in (1,\\varphi]\\), where \\(\\varphi = \\frac{1+\\sqrt{5}}{2}\\) is the golden ratio.</p> <ol> <li> <p>Malitsky, Y. (2020). Golden ratio algorithms for variational     inequalities. Mathematical Programming, 184(1), 383-410.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>The initial auxiliary point, corresponding to \\(\\mathbf{y}_0\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size value, corresponding to \\(\\chi\\)</p> </li> <li> <code>phi</code>               (<code>float</code>, default:                   <code>GOLDEN_RATIO</code> )           \u2013            <p>The golden ratio step size, corresponding to \\(\\phi\\)</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the  <code>cvxpy.Problem.solve</code>  method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#adaptive-golden-ratio-algorithm","title":"Adaptive golden ratio algorithm","text":""},{"location":"api/#monviso.VI.agraal","title":"monviso.VI.agraal","text":"<pre><code>agraal(\n    x_current: ndarray,\n    x_previous: ndarray,\n    step_size: float,\n    phi: float = GOLDEN_RATIO,\n    step_size_large: float = 1000000.0,\n    **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>The Adaptive Golden Ratio Algorithm (aGRAAL) algorithm is a variation of the Golden Ratio Algorithm (monviso.VI.graal), with adaptive step size.  Following <sup>1</sup>, let \\(\\theta_0 = 1\\), \\(\\rho = 1/\\phi + 1/\\phi^2\\), where \\(\\phi \\in (0,\\varphi]\\) and \\(\\varphi = \\frac{1+\\sqrt{5}}{2}\\) is the golden ratio.  Moreover, let \\(\\bar{\\chi} \\gg 0\\) be a constant (arbitrarily large) step-size.  Given the initial terms \\(\\mathbf{x}_0,\\mathbf{x}_1 \\in \\mathbb{R}^n\\), \\(\\mathbf{y}_0 = \\mathbf{x}_1\\), and \\(\\chi_0 &gt; 0\\), the \\(k\\)-th iterate for aGRAAL is the following:</p> \\[ \\begin{align*}  \\chi_k &amp;= \\min\\left\\{\\rho\\chi_{k-1},       \\frac{\\phi\\theta_k \\|\\mathbf{x}_k       -\\mathbf{x}_{k-1}\\|^2}{4\\chi_{k-1}\\|\\mathbf{F}(\\mathbf{x}_k)       -\\mathbf{F}(\\mathbf{x}_{k-1})\\|^2}, \\bar{\\chi}\\right\\} \\\\ \\mathbf{y}_{k+1} &amp;= \\frac{(\\phi - 1)\\mathbf{x}_k + \\phi\\mathbf{y}_k}{\\phi} \\\\ \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{y}_{k+1} - \\chi      \\mathbf{F}(\\mathbf{x}_k)) \\\\ \\theta_k &amp;= \\phi\\frac{\\chi_k}{\\chi_{k-1}}  \\end{align*} \\] <p>The convergence guarantees discussed for GRAAL also hold for aGRAAL. </p> <ol> <li> <p>Malitsky, Y. (2020). Golden ratio algorithms for variational inequalities. Mathematical Programming, 184(1), 383-410.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x_current</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>x_previous</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_1\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size initial value, corresponding to \\(\\chi_0\\)</p> </li> <li> <code>phi</code>               (<code>float</code>, default:                   <code>GOLDEN_RATIO</code> )           \u2013            <p>The golden ratio step size, corresponding to \\(\\phi\\)</p> </li> <li> <code>step_size_large</code>               (<code>float</code>, default:                   <code>1000000.0</code> )           \u2013            <p>A constant (arbitrarily) large value for the step size</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the <code>cvxpy.Problem.solve</code> method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#hybrid-golden-ratio-algorithm-i","title":"Hybrid golden ratio algorithm I","text":""},{"location":"api/#monviso.VI.hgraal_1","title":"monviso.VI.hgraal_1","text":"<pre><code>hgraal_1(\n    x_current: ndarray,\n    x_previous: ndarray,\n    step_size: float,\n    phi: float = GOLDEN_RATIO,\n    step_size_large: float = 1000000.0,\n    **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>The HGRAAL-1 algorithm is a variation of the Adaptive Golden Ratio Algorithm (monviso.VI.agraal).  Following <sup>1</sup>, let \\(\\theta_0 = 1\\), \\(\\rho = 1/\\phi + 1/\\phi^2\\), where \\(\\phi \\in (0,\\varphi]\\) and \\(\\varphi = \\frac{1+\\sqrt{5}}{2}\\) is the golden ratio.  The residual at point \\(\\mathbf{x}_k\\) is given by \\(J : \\mathbb{R}^n \\to \\mathbb{R}\\), defined as follows:</p> \\[ J(\\mathbf{x}_k) = \\|\\mathbf{x}_k - \\text{prox}_{g,\\mathcal{S}} (\\mathbf{x}_k - \\mathbf{F}(\\mathbf{x}_k))\\| \\] <p>Moreover, let \\(\\bar{\\chi} \\gg 0\\) be a constant (arbitrarily large) step-size.  Given the initial terms \\(\\mathbf{x}_0,\\mathbf{x}_1 \\in\\mathbb{R}^n\\), \\(\\mathbf{y}_0 = \\mathbf{x}_1\\), and \\(\\chi_0 &gt; 0\\), the \\(k\\)-th iterate for HGRAAL-1 is the following:</p> \\[  \\begin{align}     \\chi_k &amp;= \\min\\left\\{\\rho\\chi_{k-1},         \\frac{\\phi\\theta_k \\|\\mathbf{x}_k         -\\mathbf{x}_{k-1}\\|^2}{4\\chi_{k-1}\\|\\mathbf{F}(\\mathbf{x}_k)         -\\mathbf{F}(\\mathbf{x}_{k-1})\\|^2}, \\bar{\\chi}\\right\\} \\\\     c_k &amp;= \\left(\\langle J(\\mathbf{x}_k) - J(\\mathbf{x}_{k-1}) &gt; 0 \\rangle          \\text{ and } \\langle f_k \\rangle \\right)          \\text{ or } \\left\\langle \\min\\{J(\\mathbf{x}_{k-1}), J(\\mathbf{x}_k)\\} &lt;          J(\\mathbf{x}_k) + \\frac{1}{\\bar{k}} \\right\\rangle \\\\     f_k &amp;= \\text{not $\\langle c_k \\rangle$} \\\\     \\bar{k} &amp;= \\begin{cases} \\bar{k}+1 &amp; \\text{if $c_k$ is true} \\\\          \\bar{k} &amp; \\text{otherwise} \\end{cases} \\\\     \\mathbf{y}_{k+1} &amp;=          \\begin{cases}             \\dfrac{(\\phi - 1)\\mathbf{x}_k + \\phi\\mathbf{y}_k}{\\phi} &amp;              \\text{if $c_k$ is true} \\\\             \\mathbf{x}_k &amp; \\text{otherwise}         \\end{cases} \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{y}_{k+1} - \\chi          \\mathbf{F}(\\mathbf{x}_k)) \\\\     \\theta_k &amp;= \\phi\\frac{\\chi_k}{\\chi_{k-1}}  \\end{align} \\] <ol> <li> <p>Rahimi Baghbadorani, R., Mohajerin Esfahani, P., &amp; Grammatico, S. (2024). A hybrid algorithm for monotone variational inequalities.  (Manuscript submitted for publication).\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x_current</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>x_previous</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_1\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size initial value, corresponding to \\(\\chi_0\\)</p> </li> <li> <code>phi</code>               (<code>float</code>, default:                   <code>GOLDEN_RATIO</code> )           \u2013            <p>The golden ratio step size, corresponding to \\(\\phi\\)</p> </li> <li> <code>step_size_large</code>               (<code>float</code>, default:                   <code>1000000.0</code> )           \u2013            <p>A constant (arbitrarily) large value for the step size</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the  <code>cvxpy.Problem.solve</code>  method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"api/#hybrid-golden-ratio-algorithm-ii","title":"Hybrid golden ratio algorithm II","text":""},{"location":"api/#monviso.VI.hgraal_2","title":"monviso.VI.hgraal_2","text":"<pre><code>hgraal_2(\n    x_current: ndarray,\n    x_previous: ndarray,\n    step_size: float,\n    phi: float = GOLDEN_RATIO,\n    alpha: float = GOLDEN_RATIO,\n    step_size_large: float = 1000000.0,\n    phi_large: float = 1000000.0,\n    **cvxpy_solve_params\n) -&gt; np.ndarray\n</code></pre> <p>The pseudo-code for the iteration schema can be found at [Algorithm 2]<sup>1</sup>.</p> <ol> <li> <p>Rahimi Baghbadorani, R., Mohajerin Esfahani, P., &amp; Grammatico, S.(2024). A hybrid algorithm for monotone variational inequalities. (Manuscript submitted for publication).\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>x_current</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_0\\)</p> </li> <li> <code>x_previous</code>               (<code>ndarray</code>)           \u2013            <p>The initial point, corresponding to \\(\\mathbf{x}_1\\)</p> </li> <li> <code>step_size</code>               (<code>float</code>)           \u2013            <p>The step size initial value, corresponding to \\(\\chi_0\\)</p> </li> <li> <code>phi</code>               (<code>float</code>, default:                   <code>GOLDEN_RATIO</code> )           \u2013            <p>The golden ratio step size, corresponding to \\(\\phi\\)</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>GOLDEN_RATIO</code> )           \u2013            <p>The auxiliary parameter, corresponding to the \\(\\alpha\\) parameter.</p> </li> <li> <code>step_size_large</code>               (<code>float</code>, default:                   <code>1000000.0</code> )           \u2013            <p>A constant (arbitrarily) large value for the step size</p> </li> <li> <code>phi_large</code>               (<code>float</code>, default:                   <code>1000000.0</code> )           \u2013            <p>A constant (arbitrarily) large value for \\(\\phi\\)</p> </li> <li> <code>**cvxpy_solve_params</code>           \u2013            <p>The parameters for the <code>cvxpy.Problem.solve</code> method.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The iteration's resulting point</p> </li> </ul>"},{"location":"examples/feasibility-problem-nb/","title":"Feasibility problem","text":"In\u00a0[4]: Copied! <pre>import numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom utils import *\nfrom monviso import VI\n\nnp.random.seed(2024)\n\n# Problem data\nn, M = 1000, 2000\nc = np.random.normal(0, 100, size=(M, n))\nr = 1 - np.linalg.norm(c, axis=0)\n\n# Projection operator and VI mapping, with its Liptshitz constant\nP = lambda x: np.where(\n    np.linalg.norm(x - c) &gt; r, r * (x - c) / np.linalg.norm(x - c, axis=0), x\n)\nF = lambda x: x - P(x).mean(axis=0)\nL = 10\n\n# Define the VI\nfp = VI(n, F)\n\n# Initial points\nx0 = [np.random.rand(n) for _ in range(2)]\n\n# Solve the VI using the available algorithms\nmax_iter = 200\nfor algorithm, params in cases(x0, L, excluded={\"pg\", \"cfogda\", \"fogda\"}).items():\n    print(f\"Using: {algorithm}\")\n    sol = fp.solution(\n        algorithm,\n        params,\n        max_iter,\n        log_path=f\"logs/feasibility/{algorithm}.log\",\n    )\n\nplot_results(\n    \"logs/feasibility\",\n    \"figs/feasibility.pdf\",\n    r\"$\\|\\mathbf{x}_k \\! - \\! \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k \\! - \\! F(\\mathbf{x}_k))\\|$\",\n)\n</pre> import numpy as np import cvxpy as cp import matplotlib.pyplot as plt  from utils import * from monviso import VI  np.random.seed(2024)  # Problem data n, M = 1000, 2000 c = np.random.normal(0, 100, size=(M, n)) r = 1 - np.linalg.norm(c, axis=0)  # Projection operator and VI mapping, with its Liptshitz constant P = lambda x: np.where(     np.linalg.norm(x - c) &gt; r, r * (x - c) / np.linalg.norm(x - c, axis=0), x ) F = lambda x: x - P(x).mean(axis=0) L = 10  # Define the VI fp = VI(n, F)  # Initial points x0 = [np.random.rand(n) for _ in range(2)]  # Solve the VI using the available algorithms max_iter = 200 for algorithm, params in cases(x0, L, excluded={\"pg\", \"cfogda\", \"fogda\"}).items():     print(f\"Using: {algorithm}\")     sol = fp.solution(         algorithm,         params,         max_iter,         log_path=f\"logs/feasibility/{algorithm}.log\",     )  plot_results(     \"logs/feasibility\",     \"figs/feasibility.pdf\",     r\"$\\|\\mathbf{x}_k \\! - \\! \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k \\! - \\! F(\\mathbf{x}_k))\\|$\", ) <pre>Using: eg\nUsing: popov\nUsing: fbf\nUsing: frb\nUsing: prg\nUsing: eag\nUsing: arg\nUsing: graal\nUsing: agraal\nUsing: hgraal_1\nUsing: hgraal_2\n</pre>"},{"location":"examples/feasibility-problem-nb/#feasibility-problem","title":"Feasibility problem\u00b6","text":"<p>Let us consider $M$ balls in $\\mathbb{R}^n$, where the $i$-th ball of radius $r_i &gt; 0$ centered in $\\mathbf{c}_i \\in \\mathbb{R}^n$ is given by $\\mathcal{B}_i(\\mathbf{c}_i, r_i) \\subset \\mathbb{R}^n$. We are interested in finding a point belonging to their intersection, i.e., we want to solve the following</p> <p>$$ \\begin{equation}      \\label{eq:intersection}     \\text{find} \\ \\mathbf{x} \\ \\text{subject to} \\ \\mathbf{x} \\in \\bigcap_{i = 1}^M \\mathcal{B}_i(\\mathbf{c}_i, r_i) \\end{equation} $$</p> <p>It is straightforward to verify that the projection of a point onto $\\mathcal{B}_i(\\mathbf{c}_i,r_i)$ is evaluated as</p> <p>$$ \\begin{equation}     \\label{eq:projection}     \\mathsf{P}_i(\\mathbf{x}) :=      \\text{proj}_{\\mathcal{B}_i(\\mathbf{c}_i,r_i)}(\\mathbf{x}) =      \\begin{cases}         \\displaystyle r_i\\frac{\\mathbf{x} -          \\mathbf{c}_i}{\\|\\mathbf{x} - \\mathbf{c}_i\\|} &amp; \\text{if} \\ \\|\\mathbf{x} - \\mathbf{c}_i\\| &gt; r_i \\\\         x &amp; \\text{otherwise}     \\end{cases} \\end{equation} $$</p> <p>Due to the non-expansiveness of the projection in $\\eqref{eq:projection}$, one can find a solution for $\\eqref{eq:intersection}$ as the fixed point of the following iterate</p> <p>$$ \\begin{equation}     \\label{eq:krasnoselskii-mann}     \\mathbf{x}_{k+1} = \\mathsf{T}(\\mathbf{x}_k) = \\frac{1}{M}\\sum_{i = 1}^M\\mathsf{P}_i(\\mathbf{x}_k)  \\end{equation} $$</p> <p>which result from the well-known Krasnoselskii-Mann iterate. By letting $F = \\mathsf{I} - \\mathsf{T}$, where $\\mathsf{I}$ denotes the identity operator, the fixed point for $\\eqref{eq:krasnoselskii-mann}$ can be treated as the canonical VI [1].</p>"},{"location":"examples/feasibility-problem-nb/#references","title":"References\u00b6","text":"<p>[1] Bauschke, H. H., &amp; Borwein, J. M. (1996). On projection algorithms for solving convex feasibility problems. SIAM review, 38(3), 367-426.</p>"},{"location":"examples/linear-complementarity-nb/","title":"Linear Complementarity Problem","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport cvxpy as cp\n\nfrom utils import *\nfrom monviso import VI\n\nnp.random.seed(2024)\n\n# Problem data\nn = 5\nq = np.random.uniform(-10, 10, size=n)\nM = random_positive_definite_matrix(-10, 10, n)\n\n# Define the mapping and constraints' set\nF = lambda x: -(q + M @ x)\nL = np.linalg.norm(M, 2)\nS = [lambda x: x &gt;= 0, lambda x: -q @ x - cp.quad_form(x, M) &gt;= 0]\n\n# Define the VI and the initial(s) points\nlcp = VI(n, F, S=S)\nx0 = []\nx = cp.Variable(n)\nfor _ in range(2):\n    prob = cp.Problem(\n        cp.Minimize(np.random.rand(n) @ x),\n        constraints=[constraint(x) for constraint in S],\n    ).solve()\n    x0.append(x.value)\n\n# Solve the VI using the available algorithms\nmax_iter = 200\nfor algorithm, params in cases(x0, L, excluded={\"fogda\", \"cfogda\"}).items():\n    print(f\"Using: {algorithm}\")\n    sol = lcp.solution(\n        algorithm,\n        params,\n        max_iter,\n        log_path=f\"logs/linear-complementarity/{algorithm}.log\",\n    )\n\nplot_results(\n    \"logs/linear-complementarity\",\n    \"figs/linear-complementarity.pdf\",\n    r\"$\\|\\mathbf{x}_k \\! - \\! \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k \\! - \\! F(\\mathbf{x}_k))\\|$\",\n)\n</pre> import numpy as np import cvxpy as cp  from utils import * from monviso import VI  np.random.seed(2024)  # Problem data n = 5 q = np.random.uniform(-10, 10, size=n) M = random_positive_definite_matrix(-10, 10, n)  # Define the mapping and constraints' set F = lambda x: -(q + M @ x) L = np.linalg.norm(M, 2) S = [lambda x: x &gt;= 0, lambda x: -q @ x - cp.quad_form(x, M) &gt;= 0]  # Define the VI and the initial(s) points lcp = VI(n, F, S=S) x0 = [] x = cp.Variable(n) for _ in range(2):     prob = cp.Problem(         cp.Minimize(np.random.rand(n) @ x),         constraints=[constraint(x) for constraint in S],     ).solve()     x0.append(x.value)  # Solve the VI using the available algorithms max_iter = 200 for algorithm, params in cases(x0, L, excluded={\"fogda\", \"cfogda\"}).items():     print(f\"Using: {algorithm}\")     sol = lcp.solution(         algorithm,         params,         max_iter,         log_path=f\"logs/linear-complementarity/{algorithm}.log\",     )  plot_results(     \"logs/linear-complementarity\",     \"figs/linear-complementarity.pdf\",     r\"$\\|\\mathbf{x}_k \\! - \\! \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k \\! - \\! F(\\mathbf{x}_k))\\|$\", ) <pre>Using: pg\nUsing: eg\nUsing: popov\nUsing: fbf\nUsing: frb\nUsing: prg\nUsing: eag\nUsing: arg\nUsing: graal\nUsing: agraal\nUsing: hgraal_1\nUsing: hgraal_2\n</pre>"},{"location":"examples/linear-complementarity-nb/#linear-complementarity-problem","title":"Linear Complementarity Problem\u00b6","text":"<p>A common problem that can be cast to a VI is the linear complementarity problem [1]: given $\\mathbf{q} \\in \\mathbb{R}^n$ and $0 \\prec \\mathbf{M} \\in \\mathbb{R}^{n \\times n}$, one want to solve the following</p> <p>$$ \\begin{equation}     \\label{eq:complementarity}     \\text{find $\\mathbf{x} \\in \\mathbb{R}^n_{\\geq 0}$ s.t. $\\mathbf{y} = \\mathbf{M}\\mathbf{x} + \\mathbf{q}$, $\\mathbf{y}^\\top \\mathbf{x} = 0$} \\end{equation} $$</p> <p>By setting $F(\\mathbf{x}) = - \\mathbf{M}\\mathbf{x} - \\mathbf{q}$ and $\\mathcal{S} = \\mathbb{R}_{\\geq 0}$ it can be readily verified that each solution for $(\\mathbf{x} - \\mathbf{x}^*)^\\top F(\\mathbf{x}^*) \\geq 0$ is also a solution for $\\eqref{eq:complementarity}$.</p>"},{"location":"examples/linear-complementarity-nb/#references","title":"References\u00b6","text":"<p>[1] Harker, P. T., &amp; Pang, J. S. (1990). For the linear complementarity problem. Lectures in Applied Mathematics, 26, 265-284.</p>"},{"location":"examples/linear-quadratic-game-nb/","title":"Linear-Quadratic Dynamic Game","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport scipy as sp\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom utils import *\nfrom monviso import VI\n\nnp.random.rand(2024)\n\n# State and input sizes, number of agents, and time steps\nn, m, N, T = 13, 4, 5, 3\n\n# Problem data\nA = np.random.rand(n, n)\nB = [np.random.rand(n, m) for _ in range(N)]\nQ = [random_positive_definite_matrix(2, 4, n) for _ in range(N)]\nR = [random_positive_definite_matrix(1, 2, m) for _ in range(N)]\nP = np.random.rand(n, n)\nQ_bar = [sp.linalg.block_diag(np.kron(I(T - 1), Q[i]), P) for i in range(N)]\nG = [\n    np.kron(I(T), B[i])\n    + np.kron(\n        e(0, T),\n        np.vstack([np.linalg.matrix_power(A, t) @ B[i] for t in range(T)]),\n    )\n    for i in range(N)\n]\nH = np.vstack([np.linalg.matrix_power(A, t) for t in range(1, T + 1)])\nx0 = np.random.rand(n)\n\n# Define the mapping\nF1 = np.vstack([G[i].T @ Q_bar[i] for i in range(N)])\nF2 = np.hstack(G)\nF3 = sp.linalg.block_diag(*[np.kron(I(T), R[i]) for i in range(N)])\nF = lambda u: F1 @ (F2 @ u + H @ x0) + F3 @ u\nL = np.linalg.norm(F1 @ F2 + F3, 2) + 1\n\n# Define a constraints set for the collective input\nS = [lambda u: u &gt;= 0]\n\n# Define the VI and the initial(s) points\nlqg = VI(m * T * N, F, S=S)\nu0 = [np.random.rand(m * T * N) for _ in range(2)]\n\n# Solve the VI using the available algorithms\nmax_iter = 200\nfor algorithm, params in cases(u0, L, excluded={\"cfogda\", \"fogda\"}).items():\n    print(f\"Using: {algorithm}\")\n    sol = lqg.solution(\n        algorithm,\n        params,\n        max_iter,\n        log_path=f\"logs/linear-quadratic-game/{algorithm}.log\",\n    )\n\nplot_results(\n    \"logs/linear-quadratic-game\",\n    \"figs/linear-quadratic-game.pdf\",\n    r\"$\\|\\mathbf{x}_k \\! - \\! \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k \\! - \\! F(\\mathbf{x}_k))\\|$\",\n)\n</pre> import numpy as np import scipy as sp import cvxpy as cp import matplotlib.pyplot as plt  from utils import * from monviso import VI  np.random.rand(2024)  # State and input sizes, number of agents, and time steps n, m, N, T = 13, 4, 5, 3  # Problem data A = np.random.rand(n, n) B = [np.random.rand(n, m) for _ in range(N)] Q = [random_positive_definite_matrix(2, 4, n) for _ in range(N)] R = [random_positive_definite_matrix(1, 2, m) for _ in range(N)] P = np.random.rand(n, n) Q_bar = [sp.linalg.block_diag(np.kron(I(T - 1), Q[i]), P) for i in range(N)] G = [     np.kron(I(T), B[i])     + np.kron(         e(0, T),         np.vstack([np.linalg.matrix_power(A, t) @ B[i] for t in range(T)]),     )     for i in range(N) ] H = np.vstack([np.linalg.matrix_power(A, t) for t in range(1, T + 1)]) x0 = np.random.rand(n)  # Define the mapping F1 = np.vstack([G[i].T @ Q_bar[i] for i in range(N)]) F2 = np.hstack(G) F3 = sp.linalg.block_diag(*[np.kron(I(T), R[i]) for i in range(N)]) F = lambda u: F1 @ (F2 @ u + H @ x0) + F3 @ u L = np.linalg.norm(F1 @ F2 + F3, 2) + 1  # Define a constraints set for the collective input S = [lambda u: u &gt;= 0]  # Define the VI and the initial(s) points lqg = VI(m * T * N, F, S=S) u0 = [np.random.rand(m * T * N) for _ in range(2)]  # Solve the VI using the available algorithms max_iter = 200 for algorithm, params in cases(u0, L, excluded={\"cfogda\", \"fogda\"}).items():     print(f\"Using: {algorithm}\")     sol = lqg.solution(         algorithm,         params,         max_iter,         log_path=f\"logs/linear-quadratic-game/{algorithm}.log\",     )  plot_results(     \"logs/linear-quadratic-game\",     \"figs/linear-quadratic-game.pdf\",     r\"$\\|\\mathbf{x}_k \\! - \\! \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k \\! - \\! F(\\mathbf{x}_k))\\|$\", ) <pre>Using: pg\nUsing: eg\nUsing: popov\nUsing: fbf\nUsing: frb\nUsing: prg\nUsing: eag\nUsing: arg\nUsing: graal\nUsing: agraal\nUsing: hgraal_1\nUsing: hgraal_2\n</pre>"},{"location":"examples/linear-quadratic-game-nb/#linear-quadratic-dynamic-game","title":"Linear-Quadratic Dynamic Game\u00b6","text":"<p>As shown in Proposition 2 in [1], the receding horizon open-loop Nash equilibria (NE) can be reformulated as a non-symmetric variational inequality. Specifically, consider a set of agents $\\mathcal{N} = \\{1,\\dots,N\\}$ characterizing a state vector $\\mathbf{x}[t] \\in \\mathbb{R}^n$, whose (linear) dynamics is described as</p> <p>$$ \\begin{equation}     \\mathbf{x}[t+1] = \\mathbf{A}\\mathbf{x}[t] + \\sum_{i \\in \\mathcal{N}} \\mathbf{B}_i \\mathbf{u}_i[t] \\end{equation} $$</p> <p>for $t = 1, \\dots, T$. Each agent $i$ selfishly tries to choose $\\mathbf{u}_i[t] \\in \\mathbb{R}^m$ in order to minimize the following cost function</p> <p>$$ \\begin{equation}     J_i(\\mathbf{u}_i|\\mathbf{x}_0, \\mathbf{u}_{-i}) = \\frac{1}{2}\\sum_{t=0}^{T-1} \\|\\mathbf{x}[t|\\mathbf{x}_0, \\mathbf{u}]\\|^2_{\\mathbf{Q}_i} + \\|\\mathbf{u}_i[t] \\|^2_{\\mathbf{R}_i} \\end{equation} $$</p> <p>for some $0 \\preceq \\mathbf{Q}_i \\in \\mathbb{R}^{n \\times n}$ and $0 \\prec \\mathbf{R}_i \\in \\mathbb{R}^{m \\times m}$, with $\\mathbf{u}_{-i} = \\text{col}(\\mathbf{u}_j)_{j \\in \\mathcal{N}\\setminus \\{i\\}}$ and $\\mathbf{u}_j = \\text{col}(\\mathbf{u}_j[t])_{t=1}^T$. Moreover, $\\mathbf{u} = \\text{col}(\\mathbf{u}_i)_{i \\in \\mathcal{N}}$. The set of feasible inputs, for each agent $i \\in \\mathcal{N}$, is $\\mathcal{U}_i(\\mathbf{x}_0,\\mathbf{u}_{-i}) := \\{\\mathbf{u}_i \\in \\mathbb{R}^{mT} : \\mathbf{u}_i[t] \\in \\mathcal{U}_i(\\mathbf{u}_{-i}[t]), \\ \\forall t = 0,\\dots,T-1; \\ \\mathbf{x}[t|\\mathbf{x}_0, \\mathbf{u}] \\in \\mathcal{X}, \\ \\forall t = 1,\\dots,T\\}$, where $\\mathcal{X} \\in \\mathbb{R}^n$ is the set of feasible system states. Finally, $\\mathcal{U}(\\mathbf{x}_0) = \\{\\mathbf{u} \\in \\mathbb{R}^{mTN}: \\mathbf{u}_i \\in \\mathcal{U}(\\mathbf{x}_0,\\mathbf{u}_{-i}), \\ \\forall i \\in \\mathcal{N}\\}$. Following Definition 1 in [1], the sequence of input $\\mathbf{u}^*_i \\in \\mathcal{U}_i(\\mathbf{x}_0,\\mathbf{u}_{-i})$, for all $i \\in \\mathcal{N}$, characterizes an open-loop NE iff</p> <p>$$ \\begin{equation}     J(\\mathbf{u}^*_i|\\mathbf{x}_0,\\mathbf{u}^*_{-i}) \\leq \\inf_{\\mathbf{u}_i \\in \\mathcal{U}_i(\\mathbf{x}_0, \\mathbf{u}^*_{-i})}\\left\\{ J(\\mathbf{u}^*_i|\\mathbf{x}_0,\\mathbf{u}_{-i}) \\right\\} \\end{equation} $$</p> <p>which is satisfied by the fixed-point of the best response mapping of each agent, defined as</p> <p>$$ \\begin{equation}     \\label{eq:best_response}     \\mathbf{u}^*_i = \\underset{{\\mathbf{u}_i \\in \\mathcal{U}(\\mathbf{x}_0,\\mathbf{u}^*_{-i})}}{\\text{argmin}} J_i(\\mathbf{u}_i|\\mathbf{x}_0, \\mathbf{u}^*_{-i}), \\quad \\forall i \\in \\mathcal{N} \\end{equation} $$</p> <p>Proposition 2 in [1] states that any solution of the canonical VI is a solution for $\\eqref{eq:best_response}$ when $\\mathcal{S} = \\mathcal{U}(\\mathbf{x}_0)$ and $F : \\mathbb{R}^{mTN} \\to \\mathbb{R}^{mTN}$, defined as</p> <p>$$ \\begin{equation}     F(\\mathbf{u}) = \\text{col}(\\mathbf{G}^\\top_i \\bar{\\mathbf{Q}}_i)_{i \\in \\mathcal{N}} (\\text{row}(\\mathbf{G}_i)_{i \\in \\mathcal{N}}\\mathbf{u} + \\mathbf{H} \\mathbf{x}_0) +     \\text{blkdiag}(\\mathbf{I}_T \\otimes \\mathbf{R}_i)_{i \\in \\mathcal{N}} \\mathbf{u} \\end{equation} $$</p> <p>where, for all $i \\in \\mathcal{N}$, $\\bar{\\mathbf{Q}}_i = \\text{blkdiag}(\\mathbf{I}_{T-1} \\otimes \\mathbf{Q}_i, \\mathbf{P}_i)$, $\\mathbf{G}_i = \\mathbf{e}^\\top_{1,T} \\otimes \\text{col}(\\mathbf{A}^t_i \\mathbf{B}_i)_{t=0}^{T-1} + \\mathbf{I}_T \\otimes \\mathbf{B}_i$ and $\\mathbf{H} = \\text{col}(\\mathbf{A}^t)_{t = 1}^T$. Matrix $\\mathbf{P}_i$ results from the open-loop NE feedback synthesis as discussed in [Equation 6] [1].</p>"},{"location":"examples/linear-quadratic-game-nb/#references","title":"References\u00b6","text":"<p>[1] Benenati, E., &amp; Grammatico, S. (2024). Linear-Quadratic Dynamic Games as Receding-Horizon Variational Inequalities. arXiv preprint arXiv:2408.15703.</p>"},{"location":"examples/logistic-regression-nb/","title":"Sparse logistic regression","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom utils import *\nfrom monviso import VI\n\nnp.random.seed(2024)\n\nN, M = 500, 200\n\n# Train matrix, target vector, and regularization strength\nA = np.random.normal(size=(M, N))\nb = np.random.choice([-1, 1], size=M)\ngamma = 0.005 * np.linalg.norm(A.T @ b, np.inf)\n\n# VI mapping\nF = lambda x: -np.sum(\n    (A.T * np.tile(b, (N, 1))) * np.exp(-b * (A @ x)) / (1 + np.exp(-b * (A @ x))),\n    axis=1,\n)\ng = lambda x: gamma * cp.norm(x, 1)\nL = 1.5\n\n# Define the VI problem\nslr = VI(N, F, g)\n\n# Initial points\nx0 = [np.random.rand(N) for _ in range(2)]\n\n# Solve the VI using the available algorithms\nmax_iter = 200\nfor algorithm, params in cases(x0, L, excluded={\"pg\", \"cfogda\"}).items():\n    print(f\"Using: {algorithm}\")\n    sol = slr.solution(\n        algorithm,\n        params,\n        max_iter,\n        log_path=f\"logs/logistic-regression/{algorithm}.log\",\n    )\n\nplot_results(\n    \"logs/logistic-regression\",\n    \"figs/logistic-regression.pdf\",\n    r\"$\\|F(\\mathbf{x}_k))\\|$\",\n)\n</pre> import numpy as np import cvxpy as cp import matplotlib.pyplot as plt  from utils import * from monviso import VI  np.random.seed(2024)  N, M = 500, 200  # Train matrix, target vector, and regularization strength A = np.random.normal(size=(M, N)) b = np.random.choice([-1, 1], size=M) gamma = 0.005 * np.linalg.norm(A.T @ b, np.inf)  # VI mapping F = lambda x: -np.sum(     (A.T * np.tile(b, (N, 1))) * np.exp(-b * (A @ x)) / (1 + np.exp(-b * (A @ x))),     axis=1, ) g = lambda x: gamma * cp.norm(x, 1) L = 1.5  # Define the VI problem slr = VI(N, F, g)  # Initial points x0 = [np.random.rand(N) for _ in range(2)]  # Solve the VI using the available algorithms max_iter = 200 for algorithm, params in cases(x0, L, excluded={\"pg\", \"cfogda\"}).items():     print(f\"Using: {algorithm}\")     sol = slr.solution(         algorithm,         params,         max_iter,         log_path=f\"logs/logistic-regression/{algorithm}.log\",     )  plot_results(     \"logs/logistic-regression\",     \"figs/logistic-regression.pdf\",     r\"$\\|F(\\mathbf{x}_k))\\|$\", )  <pre>Using: eg\nUsing: popov\nUsing: fbf\nUsing: frb\nUsing: prg\nUsing: eag\nUsing: arg\nUsing: fogda\nUsing: graal\nUsing: agraal\nUsing: hgraal_1\nUsing: hgraal_2\n</pre>"},{"location":"examples/logistic-regression-nb/#sparse-logistic-regression","title":"Sparse logistic regression\u00b6","text":"<p>Consider a dataset of $M$ rows and $N$ columns, so that $\\mathbf{A} = \\text{col}(\\mathbf{a}^\\top_i)_{i =1}^M \\in \\mathbb{R}^{M \\times N}$ is the dataset matrix, and $\\mathbf{a}_i \\in \\mathbb{R}^{N}$ is the $i$-th features vector for the $i$-th dataset row. Moreover, let $\\mathbf{b} \\in \\mathbb{R}^M$ be the target vector, so that $b_i \\in \\{-1,1\\}$ is the (binary) ground truth for the $i$-th data entry. The sparse logistic regression consists of finding the weight vector $\\mathbf{x} \\in \\mathbb{R}^N$ that minimizes the following loss function [1]</p> <p>$$ \\begin{align}     \\label{eq:regression}         f(\\mathbf{x}) := \\sum_{i = 1}^M \\log\\left(1 + \\frac{1}{\\exp(b_i \\mathbf{a}^\\top_i \\mathbf{x})} \\right) + \\gamma \\|\\mathbf{x}\\|_1         \\\\ \\nonumber         = \\underbrace{\\mathbf{1}^\\top_M \\log(1 + \\exp(-\\mathbf{b} \\odot \\mathbf{A} \\mathbf{x}))}_{=:s(\\mathbf{x})} + \\underbrace{\\gamma \\|\\mathbf{x}\\|_1}_{=:g(\\mathbf{x})}  \\end{align} $$</p> <p>where $\\gamma \\in \\mathbb{R}_{&gt; 0}$ is the $\\ell_1$-regulation strength. The gradient for $s(\\cdot)$, $\\nabla s_\\mathbf{x}(\\mathbf{x})$, is calculated as</p> <p>$$ \\begin{equation}     F(\\mathbf{x}) = \\nabla s_\\mathbf{x}(\\mathbf{x}) = -\\frac{\\mathbf{A}^\\top \\odot (\\mathbf{1}_N \\otimes \\mathbf{b}^\\top) \\odot \\exp(-\\mathbf{b} \\odot \\mathbf{A} \\mathbf{x})}{1 + \\exp(-\\mathbf{b} \\odot \\mathbf{A} \\mathbf{x})} \\mathbf{1}_M \\end{equation} $$</p> <p>The problem of finding the minimizer for $\\eqref{eq:regression}$ can be cast as a canonical VI, with $F(\\mathbf{x}) := \\nabla s_\\mathbf{x}(\\mathbf{x})$.</p>"},{"location":"examples/logistic-regression-nb/#references","title":"References\u00b6","text":"<p>[1] Mishchenko, K. (2023). Regularized Newton method with global convergence. SIAM Journal on Optimization, 33(3), 1440-1462.</p>"},{"location":"examples/markov-decision-process-nb/","title":"Markov Decision Process","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom utils import *\nfrom monviso import VI\n\nnp.random.seed(2024)\n\n# Number of states and actions\nnum_X, num_A = 20, 10\n\n# Discount factor\ngamma = 0.8\n\n# Transition probabilities\nP = np.random.rand(num_X, num_A, num_X)\nP /= P.sum(2, keepdims=True)\n\n# Reward\nR = np.random.rand(num_X, num_X)\n\n# Bellman operator (as fixed point) and VI mapping\nT = lambda v: np.einsum(\"ijk,ik -&gt; ij\", P, R + gamma * v[None, :]).max(1)\nF = lambda x: x - T(x)\nL = 3\n\n# Create the VI and the initial solution(s)\nmdp = VI(num_X, F)\nx0 = [np.random.rand(num_X) for _ in range(2)]\n\n# Solve the VI using the available algorithms\nmax_iter = 200\nfor algorithm, params in cases(x0, L, excluded={\"pg\", \"cfogda\"}).items():\n    print(f\"Using: {algorithm}\")\n    sol = mdp.solution(\n        algorithm,\n        params,\n        max_iter,\n        eval_func=lambda x: np.linalg.norm(F(x), 2),\n        log_path=f\"logs/markov-decision-process/{algorithm}.log\",\n    )\n\nplot_results(\n    \"logs/markov-decision-process\",\n    \"figs/markov-decision-process.pdf\",\n    r\"$\\|F(\\mathbf{x}_k))\\|$\",\n)\n</pre> import numpy as np import cvxpy as cp import matplotlib.pyplot as plt  from utils import * from monviso import VI  np.random.seed(2024)  # Number of states and actions num_X, num_A = 20, 10  # Discount factor gamma = 0.8  # Transition probabilities P = np.random.rand(num_X, num_A, num_X) P /= P.sum(2, keepdims=True)  # Reward R = np.random.rand(num_X, num_X)  # Bellman operator (as fixed point) and VI mapping T = lambda v: np.einsum(\"ijk,ik -&gt; ij\", P, R + gamma * v[None, :]).max(1) F = lambda x: x - T(x) L = 3  # Create the VI and the initial solution(s) mdp = VI(num_X, F) x0 = [np.random.rand(num_X) for _ in range(2)]  # Solve the VI using the available algorithms max_iter = 200 for algorithm, params in cases(x0, L, excluded={\"pg\", \"cfogda\"}).items():     print(f\"Using: {algorithm}\")     sol = mdp.solution(         algorithm,         params,         max_iter,         eval_func=lambda x: np.linalg.norm(F(x), 2),         log_path=f\"logs/markov-decision-process/{algorithm}.log\",     )  plot_results(     \"logs/markov-decision-process\",     \"figs/markov-decision-process.pdf\",     r\"$\\|F(\\mathbf{x}_k))\\|$\", )  <pre>Using: eg\nUsing: popov\nUsing: fbf\nUsing: frb\nUsing: prg\nUsing: eag\nUsing: arg\nUsing: fogda\nUsing: graal\nUsing: agraal\nUsing: hgraal_1\nUsing: hgraal_2\n</pre>"},{"location":"examples/markov-decision-process-nb/#markov-decision-process","title":"Markov Decision Process\u00b6","text":"<p>A stationary discrete Markov Decision Process (MDP) is characterized by the tuple $(\\mathcal{X},\\mathcal{A},\\mathbb{P},r,\\gamma)$, where</p> <ol> <li>$\\mathcal{X}$ is the (finite countable) set of states;</li> <li>$\\mathcal{A}$ is the (finite countable) set of actions;</li> <li>$P : \\mathcal{X} \\times \\mathcal{A} \\times \\mathcal{X} \\to [0,1]$ is the transition probability function, such that $P(x,a,x^+)$ is the probability of ending up in state $x^+ \\in \\mathcal{S}$ from state $x \\in \\mathcal{X}$ when taking action $a \\in \\mathcal{A}$;</li> <li>$r : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ is the reward function, so that $r(x,x^+)$ returns the reward for transitioning from state $x \\in \\mathcal{X}$ to state $x^+ \\in \\mathcal{X}$;</li> <li>$\\gamma \\in \\mathbb{R}_{&gt; 0}$ is a discount factor [1].</li> </ol> <p>The aim is to find a policy, i.e., a function $\\pi : \\mathcal{S} \\to \\mathcal{A}$, returning the best action for any given state. A solution concept for MDP is the value function, $v^{\\pi} : \\mathcal{S} \\to \\mathbb{R}$, defined as $$ \\begin{equation}     \\label{eq:bellman}     v^{\\pi}(x) = \\overbrace{\\sum_{x^+ \\in \\mathcal{X}} P(x,\\pi(x),x^+) \\left( r(x,x^+) + \\gamma v(x^+) \\right)}^{=:\\mathsf{T}(v^{\\pi})} \\end{equation} $$</p> <p>returning the \"goodness\" of policy $\\pi$. The expression in $\\eqref{eq:bellman}$ is known as Bellman equation, and can be expressed as an operator of $v^{\\pi}$, i.e., $\\mathsf{T}[v^\\pi(s)] =: \\mathsf{T}(v^{\\pi})$. It can be shown that the value function yielded by the optimal policy, $v^*$, results from the fixed-point problem $v^* = \\mathsf{T}(v^*)$. Therefore, the latter can be formulated as a canonical VI, with $F = \\mathsf{I} - \\mathsf{T}$.</p>"},{"location":"examples/markov-decision-process-nb/#references","title":"References\u00b6","text":"<p>[1] Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.</p>"},{"location":"examples/quickstart-nb/","title":"Quickstart","text":"In\u00a0[4]: Copied! <pre>import numpy as np\nimport cvxpy as cp\n\nimport matplotlib.pyplot as plt\n\nfrom monviso import VI\n\nnp.random.seed(2024)\n\n# Create the problem data\nn, m = 30, 40\nH = np.random.uniform(2, 10, size=(n, n))\nA = np.random.uniform(45, 50, size=(m, n))\nb = np.random.uniform(3, 7, size=(m,))\n\n# Make H positive definite\nH = H @ H.T\n\n# Lipschitz and strong monotonicity constants\nmu = np.linalg.eigvals(H).min()\nL = np.linalg.norm(H, 2)\n\n# Define F, g, and S\nF = lambda x: H @ x\ng = lambda x: cp.norm(x)\nS = [lambda x: A @ x &lt;= b]\n\n# Define and solve the VI\nvi = VI(n, F, g, S)\n\nx0 = np.random.uniform(4, 5, n)\nalgorithm_params = {\"x\": x0, \"step_size\": 2 * mu / L**2}\nsol = vi.solution(\n    \"pg\", algorithm_params, max_iters=25, eval_tol=-np.inf, log_path=\"logs/quickstart/result.log\"\n)\n\n# Plot the residual\nresidual = np.genfromtxt(\n    \"logs/quickstart/result.log\", delimiter=\",\", skip_header=1, usecols=1\n)\nfig, ax = plt.subplots(figsize=(6.4, 3))\nax.plot(residual)\n\nax.grid(True, alpha=0.2)\nax.set_xlabel(\"Iterations ($k$)\")\nax.set_ylabel(\n    r\"$\\|\\mathbf{x}_k - \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k - F(\\mathbf{x}_k))\\|$\"\n)\nplt.show()\n</pre> import numpy as np import cvxpy as cp  import matplotlib.pyplot as plt  from monviso import VI  np.random.seed(2024)  # Create the problem data n, m = 30, 40 H = np.random.uniform(2, 10, size=(n, n)) A = np.random.uniform(45, 50, size=(m, n)) b = np.random.uniform(3, 7, size=(m,))  # Make H positive definite H = H @ H.T  # Lipschitz and strong monotonicity constants mu = np.linalg.eigvals(H).min() L = np.linalg.norm(H, 2)  # Define F, g, and S F = lambda x: H @ x g = lambda x: cp.norm(x) S = [lambda x: A @ x &lt;= b]  # Define and solve the VI vi = VI(n, F, g, S)  x0 = np.random.uniform(4, 5, n) algorithm_params = {\"x\": x0, \"step_size\": 2 * mu / L**2} sol = vi.solution(     \"pg\", algorithm_params, max_iters=25, eval_tol=-np.inf, log_path=\"logs/quickstart/result.log\" )  # Plot the residual residual = np.genfromtxt(     \"logs/quickstart/result.log\", delimiter=\",\", skip_header=1, usecols=1 ) fig, ax = plt.subplots(figsize=(6.4, 3)) ax.plot(residual)  ax.grid(True, alpha=0.2) ax.set_xlabel(\"Iterations ($k$)\") ax.set_ylabel(     r\"$\\|\\mathbf{x}_k - \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k - F(\\mathbf{x}_k))\\|$\" ) plt.show()"},{"location":"examples/quickstart-nb/#quickstart","title":"Quickstart\u00b6","text":"<p>Let $F(\\mathbf{x}) = \\mathbf{H} \\mathbf{x}$ for some $\\mathbf{H} \\succ 0$, $g(\\mathbf{x}) = \\|\\mathbf{x}\\|_1$, and $\\mathcal{S} = \\{\\mathbf{x} \\in \\mathbb{R}^n : \\mathbf{A} \\mathbf{x} \\leq \\mathbf{b}\\}$, for some $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^n$. It is straightforward to verify that $F(\\cdot)$ is strongly monotone with $\\mu = \\lambda_{\\min}(\\mathbf{H})$ and Lipschitz with $L = \\|\\mathbf{H}\\|_2$. The solution of the VI in can be implemented using <code>monviso</code> as follows</p>"},{"location":"examples/skew-symmetric-nb/","title":"Skew-symmetric operator","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport scipy as sp\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom utils import *\nfrom monviso import VI\n\nnp.random.rand(2024)\n\nM, N = 20, 10\n\n# Create the problem variables\nBs = [random_positive_definite_matrix(0, 1, N) for _ in range(M)]\nA = sp.linalg.block_diag(*[np.tril(B) - np.triu(B) for B in Bs])\n\nF = lambda x: A @ x\nL = np.linalg.norm(A, 2)\n\n# Create the VI and the initial solution(s)\nsso = VI(N * M, F)\nx0 = [np.random.rand(N * M) for _ in range(2)]\n\n# Solve the VI using the available algorithms\nmax_iter = 200\nfor algorithm, params in cases(x0, L, excluded={\"pg\", \"cfogda\"}).items():\n    print(f\"Using: {algorithm}\")\n    sol = sso.solution(\n        algorithm,\n        params,\n        max_iter,\n        eval_func=lambda x: np.linalg.norm(F(x), 2),\n        log_path=f\"logs/skew-symmetric/{algorithm}.log\",\n    )\n\nplot_results(\n    \"logs/skew-symmetric\",\n    \"figs/skew-symmetric.pdf\",\n    r\"$\\|F(\\mathbf{x}_k))\\|$\",\n)\n</pre> import numpy as np import scipy as sp import cvxpy as cp import matplotlib.pyplot as plt  from utils import * from monviso import VI  np.random.rand(2024)  M, N = 20, 10  # Create the problem variables Bs = [random_positive_definite_matrix(0, 1, N) for _ in range(M)] A = sp.linalg.block_diag(*[np.tril(B) - np.triu(B) for B in Bs])  F = lambda x: A @ x L = np.linalg.norm(A, 2)  # Create the VI and the initial solution(s) sso = VI(N * M, F) x0 = [np.random.rand(N * M) for _ in range(2)]  # Solve the VI using the available algorithms max_iter = 200 for algorithm, params in cases(x0, L, excluded={\"pg\", \"cfogda\"}).items():     print(f\"Using: {algorithm}\")     sol = sso.solution(         algorithm,         params,         max_iter,         eval_func=lambda x: np.linalg.norm(F(x), 2),         log_path=f\"logs/skew-symmetric/{algorithm}.log\",     )  plot_results(     \"logs/skew-symmetric\",     \"figs/skew-symmetric.pdf\",     r\"$\\|F(\\mathbf{x}_k))\\|$\", )  <pre>Using: eg\nUsing: popov\nUsing: fbf\nUsing: frb\nUsing: prg\nUsing: eag\nUsing: arg\nUsing: fogda\nUsing: graal\nUsing: agraal\nUsing: hgraal_1\nUsing: hgraal_2\n</pre>"},{"location":"examples/skew-symmetric-nb/#skew-symmetric-operator","title":"Skew-symmetric operator\u00b6","text":"<p>A simple example of monotone operator that is not (even locally) strongly monotone is the skewed-symmetric operator [1], $F : \\mathbb{R}^{MN} \\to \\mathbb{R}^{MN}$, which is described as follows</p> <p>$$ \\begin{equation}     F(\\mathbf{x}) = \\begin{bmatrix} \\mathbf{A}_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\mathbf{A}_M \\end{bmatrix} \\mathbf{x} \\end{equation} $$</p> <p>for a given $M \\in \\mathbb{N}$, where $\\mathbf{A}_i = \\text{tril}(\\mathbf{B}_i) - \\text{triu}(\\mathbf{B}_i)$, for some arbitrary $0 \\preceq \\mathbf{B}_i \\in \\mathbb{R}^{N \\times N}$, for all $i = 1, \\dots, M$.</p>"},{"location":"examples/skew-symmetric-nb/#references","title":"References\u00b6","text":"<p>[1] Bauschke, H. H., &amp; Combettes, P. L. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.</p>"},{"location":"examples/utils/","title":"Utils","text":"In\u00a0[\u00a0]: Copied! <pre>import glob\nfrom pathlib import Path\n</pre> import glob from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>I = np.eye\ne = lambda k, n: I(n)[:, k]\n</pre> I = np.eye e = lambda k, n: I(n)[:, k] In\u00a0[\u00a0]: Copied! <pre>GOLDEN_RATIO = 0.5 * (np.sqrt(5) + 1)\nALGORITHM_NAMES = (\n    \"pg\",\n    \"eg\",\n    \"popov\",\n    \"fbf\",\n    \"frb\",\n    \"prg\",\n    \"eag\",\n    \"arg\",\n    \"fogda\",\n    \"cfogda\",\n    \"graal\",\n    \"agraal\",\n    \"hgraal_1\",\n    \"hgraal_2\",\n)\nTAB20 = plt.get_cmap(\"tab20\")\nCOLORS = {name: TAB20.colors[i] for i, name in enumerate(ALGORITHM_NAMES)}\n</pre> GOLDEN_RATIO = 0.5 * (np.sqrt(5) + 1) ALGORITHM_NAMES = (     \"pg\",     \"eg\",     \"popov\",     \"fbf\",     \"frb\",     \"prg\",     \"eag\",     \"arg\",     \"fogda\",     \"cfogda\",     \"graal\",     \"agraal\",     \"hgraal_1\",     \"hgraal_2\", ) TAB20 = plt.get_cmap(\"tab20\") COLORS = {name: TAB20.colors[i] for i, name in enumerate(ALGORITHM_NAMES)} In\u00a0[\u00a0]: Copied! <pre>def random_positive_definite_matrix(upper_bound, lower_bound, size):\n    M = np.random.uniform(upper_bound, lower_bound, size=(size, size))\n    M = (M + M.T) / 2  # Make symmetric\n    # Make positive semidefinite (through Gershgorin circle theorem)\n    centers = np.diag(M)\n    M += I(size) * (np.abs(M[np.argmin(centers), :]).sum() + np.abs(centers.min()))\n    return M\n</pre> def random_positive_definite_matrix(upper_bound, lower_bound, size):     M = np.random.uniform(upper_bound, lower_bound, size=(size, size))     M = (M + M.T) / 2  # Make symmetric     # Make positive semidefinite (through Gershgorin circle theorem)     centers = np.diag(M)     M += I(size) * (np.abs(M[np.argmin(centers), :]).sum() + np.abs(centers.min()))     return M In\u00a0[\u00a0]: Copied! <pre>def cases(x0, L, z=None, only={}, excluded={}):\n    cases_list = {\n        \"pg\": {\"x\": x0[0], \"step_size\": 2 / L**2},\n        \"eg\": {\"x\": x0[0], \"step_size\": 1 / L},\n        \"popov\": {\"x\": x0[0], \"y\": x0[1], \"step_size\": 1 / (2 * L)},\n        \"fbf\": {\"x\": x0[0], \"step_size\": 1 / L},\n        \"frb\": {\"x_current\": x0[0], \"x_previous\": x0[1], \"step_size\": 1 / (2 * L)},\n        \"prg\": {\n            \"x_current\": x0[0],\n            \"x_previous\": x0[1],\n            \"step_size\": (np.sqrt(2) - 1) / L,\n        },\n        \"eag\": {\"x\": x0[0], \"step_size\": (np.sqrt(2) - 1) / L},\n        \"arg\": {\n            \"x_current\": x0[0],\n            \"x_previous\": x0[1],\n            \"step_size\": 1 / (np.sqrt(3) * L),\n        },\n        \"fogda\": {\n            \"x_current\": x0[0],\n            \"x_previous\": x0[1],\n            \"y\": x0[0],\n            \"step_size\": 1 / (4 * L),\n        },\n        \"cfogda\": {\n            \"x_current\": x0[0],\n            \"x_previous\": x0[1],\n            \"y\": x0[0],\n            \"z\": z,\n            \"step_size\": (np.sqrt(2) - 1) / L,\n        },\n        \"graal\": {\"x\": x0[0], \"y\": x0[1], \"step_size\": GOLDEN_RATIO / (2 * L)},\n        \"agraal\": {\n            \"x_current\": x0[1],\n            \"x_previous\": x0[0],\n            \"step_size\": GOLDEN_RATIO / (2 * L),\n        },\n        \"hgraal_1\": {\n            \"x_current\": x0[1],\n            \"x_previous\": x0[0],\n            \"step_size\": GOLDEN_RATIO / (2 * L),\n        },\n        \"hgraal_2\": {\n            \"x_current\": x0[1],\n            \"x_previous\": x0[0],\n            \"step_size\": GOLDEN_RATIO / (2 * L),\n        },\n    }\n\n    only = cases_list.keys() if not only else only\n    return {\n        algorithm_name: cases_list[algorithm_name]\n        for algorithm_name in only\n        if algorithm_name not in excluded\n    }\n</pre> def cases(x0, L, z=None, only={}, excluded={}):     cases_list = {         \"pg\": {\"x\": x0[0], \"step_size\": 2 / L**2},         \"eg\": {\"x\": x0[0], \"step_size\": 1 / L},         \"popov\": {\"x\": x0[0], \"y\": x0[1], \"step_size\": 1 / (2 * L)},         \"fbf\": {\"x\": x0[0], \"step_size\": 1 / L},         \"frb\": {\"x_current\": x0[0], \"x_previous\": x0[1], \"step_size\": 1 / (2 * L)},         \"prg\": {             \"x_current\": x0[0],             \"x_previous\": x0[1],             \"step_size\": (np.sqrt(2) - 1) / L,         },         \"eag\": {\"x\": x0[0], \"step_size\": (np.sqrt(2) - 1) / L},         \"arg\": {             \"x_current\": x0[0],             \"x_previous\": x0[1],             \"step_size\": 1 / (np.sqrt(3) * L),         },         \"fogda\": {             \"x_current\": x0[0],             \"x_previous\": x0[1],             \"y\": x0[0],             \"step_size\": 1 / (4 * L),         },         \"cfogda\": {             \"x_current\": x0[0],             \"x_previous\": x0[1],             \"y\": x0[0],             \"z\": z,             \"step_size\": (np.sqrt(2) - 1) / L,         },         \"graal\": {\"x\": x0[0], \"y\": x0[1], \"step_size\": GOLDEN_RATIO / (2 * L)},         \"agraal\": {             \"x_current\": x0[1],             \"x_previous\": x0[0],             \"step_size\": GOLDEN_RATIO / (2 * L),         },         \"hgraal_1\": {             \"x_current\": x0[1],             \"x_previous\": x0[0],             \"step_size\": GOLDEN_RATIO / (2 * L),         },         \"hgraal_2\": {             \"x_current\": x0[1],             \"x_previous\": x0[0],             \"step_size\": GOLDEN_RATIO / (2 * L),         },     }      only = cases_list.keys() if not only else only     return {         algorithm_name: cases_list[algorithm_name]         for algorithm_name in only         if algorithm_name not in excluded     } In\u00a0[\u00a0]: Copied! <pre>def plot_results(log_path, fig_path, ylabel):\n    fig, ax = plt.subplots(figsize=(3.5 / 2, 1.8), layout=\"constrained\")\n    # plt.yscale(\"log\")\n    for log_file in glob.glob(f\"{log_path}/*.log\"):\n        algorithm_name = Path(log_file).stem\n        eval_func_value = np.genfromtxt(\n            f\"{log_path}/{algorithm_name}.log\", delimiter=\",\", skip_header=1, usecols=1\n        )\n        ax.plot(\n            eval_func_value, lw=0.8, label=algorithm_name, color=COLORS[algorithm_name]\n        )\n\n    plt.ticklabel_format(style=\"sci\", axis=\"y\", scilimits=(0, 0))\n    plt.grid(True, alpha=0.2)\n    ax.set_xlabel(\"Iterations ($k$)\")\n    ax.set_ylabel(ylabel)\n    plt.savefig(fig_path)\n    plt.show(block=False)\n</pre> def plot_results(log_path, fig_path, ylabel):     fig, ax = plt.subplots(figsize=(3.5 / 2, 1.8), layout=\"constrained\")     # plt.yscale(\"log\")     for log_file in glob.glob(f\"{log_path}/*.log\"):         algorithm_name = Path(log_file).stem         eval_func_value = np.genfromtxt(             f\"{log_path}/{algorithm_name}.log\", delimiter=\",\", skip_header=1, usecols=1         )         ax.plot(             eval_func_value, lw=0.8, label=algorithm_name, color=COLORS[algorithm_name]         )      plt.ticklabel_format(style=\"sci\", axis=\"y\", scilimits=(0, 0))     plt.grid(True, alpha=0.2)     ax.set_xlabel(\"Iterations ($k$)\")     ax.set_ylabel(ylabel)     plt.savefig(fig_path)     plt.show(block=False) In\u00a0[\u00a0]: Copied! <pre># Plot settings\nlatex_preamble = [\n    r\"\\usepackage{amsfonts}\",\n    r\"\\usepackage{amssymb}\",\n    r\"\\usepackage{amsmath}\",\n]\n</pre> # Plot settings latex_preamble = [     r\"\\usepackage{amsfonts}\",     r\"\\usepackage{amssymb}\",     r\"\\usepackage{amsmath}\", ] In\u00a0[\u00a0]: Copied! <pre>plt.rcParams.update(\n    {\n        \"grid.alpha\": 0.5,\n        \"axes.spines.right\": False,\n        \"axes.spines.top\": False,\n        \"legend.frameon\": False,\n        \"ytick.labelsize\": 6,\n        \"xtick.labelsize\": 6,\n        \"font.size\": 8,\n        \"font.family\": \"sans\",\n        \"font.sans-serif\": [\"Computer Modern Roman\"],\n        \"text.usetex\": True,\n        \"text.latex.preamble\": \"\".join(latex_preamble),\n    }\n)\n</pre> plt.rcParams.update(     {         \"grid.alpha\": 0.5,         \"axes.spines.right\": False,         \"axes.spines.top\": False,         \"legend.frameon\": False,         \"ytick.labelsize\": 6,         \"xtick.labelsize\": 6,         \"font.size\": 8,         \"font.family\": \"sans\",         \"font.sans-serif\": [\"Computer Modern Roman\"],         \"text.usetex\": True,         \"text.latex.preamble\": \"\".join(latex_preamble),     } ) In\u00a0[\u00a0]: Copied! <pre># Plot the legend only\nfig, ax = plt.subplots(figsize=(2.1, 0.05))\nfor name, color in COLORS.items():\n    ax.plot(0, 0, label=name, color=color)\n</pre> # Plot the legend only fig, ax = plt.subplots(figsize=(2.1, 0.05)) for name, color in COLORS.items():     ax.plot(0, 0, label=name, color=color) In\u00a0[\u00a0]: Copied! <pre>plt.axis(\"off\")\nplt.legend(\n    bbox_to_anchor=(0, 1.02, 1, 0.2),\n    loc=\"lower left\",\n    mode=\"expand\",\n    borderaxespad=0,\n    ncol=2,\n)\n</pre> plt.axis(\"off\") plt.legend(     bbox_to_anchor=(0, 1.02, 1, 0.2),     loc=\"lower left\",     mode=\"expand\",     borderaxespad=0,     ncol=2, ) In\u00a0[\u00a0]: Copied! <pre>plt.savefig(\"figs/legend.pdf\", bbox_inches=\"tight\")\n</pre> plt.savefig(\"figs/legend.pdf\", bbox_inches=\"tight\")"},{"location":"examples/zero-sum-game-nb/","title":"Two Players Zero-Sum Game","text":"In\u00a0[2]: Copied! <pre>import numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom utils import *\nfrom monviso import VI\n\nnp.random.seed(2024)\n\nn1, n2 = 50, 50\n\n# Game matrix\nH = np.random.rand(n1, n2)\nH_block = np.block([[np.zeros((n1, n2)), H], [-H.T, np.zeros((n1, n2))]])\n\n# VI operators with Liptshitz constant\nF = lambda x: H_block @ x\nL = np.linalg.norm(H_block, 2)\n\n# Simplex constraints' set\nx = cp.Variable(n1 + n2)\nS = [lambda x: cp.sum(x[:n1]) == 1, lambda x: cp.sum(x[n1:]) == 1]\n\n# Define the two-players zero sum game as a Variational Inequality\ntpzsg = VI(n1 + n2, F, S=S)\n\n# Create two initial (feasible) points\nx0 = []\nfor i in range(2):\n    x0.append(np.random.rand(n1 + n2))\n    x0[i][:n1] /= x0[i][:n1].sum()\n    x0[i][n1:] /= x0[i][n1:].sum()\n\n# Solve the VI using the available algorithms\nmax_iter = 200\nfor algorithm, params in cases(x0, L, excluded={\"pg\", \"fogda\", \"cfogda\"}).items():\n    print(f\"Using: {algorithm}\")\n    sol = tpzsg.solution(\n        algorithm,\n        params,\n        max_iter,\n        log_path=f\"logs/zero-sum-game/{algorithm}.log\",\n    )\n\nplot_results(\n    \"logs/zero-sum-game\",\n    \"figs/zero-sum-game.pdf\",\n    r\"$\\|\\mathbf{x}_k \\! - \\! \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k \\! - \\! F(\\mathbf{x}_k))\\|$\",\n)\n</pre> import numpy as np import cvxpy as cp import matplotlib.pyplot as plt  from utils import * from monviso import VI  np.random.seed(2024)  n1, n2 = 50, 50  # Game matrix H = np.random.rand(n1, n2) H_block = np.block([[np.zeros((n1, n2)), H], [-H.T, np.zeros((n1, n2))]])  # VI operators with Liptshitz constant F = lambda x: H_block @ x L = np.linalg.norm(H_block, 2)  # Simplex constraints' set x = cp.Variable(n1 + n2) S = [lambda x: cp.sum(x[:n1]) == 1, lambda x: cp.sum(x[n1:]) == 1]  # Define the two-players zero sum game as a Variational Inequality tpzsg = VI(n1 + n2, F, S=S)  # Create two initial (feasible) points x0 = [] for i in range(2):     x0.append(np.random.rand(n1 + n2))     x0[i][:n1] /= x0[i][:n1].sum()     x0[i][n1:] /= x0[i][n1:].sum()  # Solve the VI using the available algorithms max_iter = 200 for algorithm, params in cases(x0, L, excluded={\"pg\", \"fogda\", \"cfogda\"}).items():     print(f\"Using: {algorithm}\")     sol = tpzsg.solution(         algorithm,         params,         max_iter,         log_path=f\"logs/zero-sum-game/{algorithm}.log\",     )  plot_results(     \"logs/zero-sum-game\",     \"figs/zero-sum-game.pdf\",     r\"$\\|\\mathbf{x}_k \\! - \\! \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k \\! - \\! F(\\mathbf{x}_k))\\|$\", )  <pre>Using: eg\nUsing: popov\nUsing: fbf\nUsing: frb\nUsing: prg\nUsing: eag\nUsing: arg\nUsing: graal\nUsing: agraal\nUsing: hgraal_1\nUsing: hgraal_2\n</pre>"},{"location":"examples/zero-sum-game-nb/#two-players-zero-sum-game","title":"Two Players Zero-Sum Game\u00b6","text":"<p>Many example of non-cooperative behavior between two adversarial agents can be modelled through zero-sum games [1]. Let us consider vectors $\\mathbf{x}_i \\in \\Delta_i$ as the decision variable of the $i$-th player, with $i \\in \\{1,2\\}$, where $\\Delta_i \\subset \\mathbb{R}^{n_i}$ is the simplex constraints set defined as $\\Delta_i := \\{\\mathbf{x} \\in \\mathbb{R}^{n_i} : \\mathbf{1}^\\top \\mathbf{x} = 1\\}$, for all $i \\in \\{1,2\\}$. Let $\\mathbf{x} := \\text{col}(\\mathbf{x}_i)_{i = 1}^2$. The players try to solve the following problem</p> <p>$$ \\begin{equation}     \\min_{\\mathbf{x}_1 \\in \\Delta_1} \\max_{\\mathbf{x}_2 \\in \\Delta_2} \\Phi(\\mathbf{x}_1, \\mathbf{x}_2) \\end{equation} $$</p> <p>whose (Nash) equilibrium solution is achieved for $\\mathbf{x}^*` satisfying the following</p> <p>$$ \\begin{equation}     \\label{eq:saddle}     \\Phi(\\mathbf{x}^*_1, \\mathbf{x}_2) \\leq \\Phi(\\mathbf{x}^*_1, \\mathbf{x}^*_2) \\leq \\Phi(\\mathbf{x}_1, \\mathbf{x}^*_2), \\quad \\forall \\mathbf{x} \\in \\Delta_1 \\times \\Delta_2 \\end{equation} $$</p> <p>For the sake of simplicity, we consider $\\Phi(\\mathbf{x}_1, \\mathbf{x}_2) := \\mathbf{x}^\\top_1 \\mathbf{H} \\mathbf{x}_2$, for some $\\mathbf{H} \\in \\mathbb{R}^{n_1 \\times n_2}$. Doing so, the equilibrium condition in the previous equation can be written as a VI, with the mapping $F : \\mathbb{R}^{n_1 + n_2} \\to \\mathbb{R}^{n_1 + n_2}$ defined as</p> <p>$$ \\begin{equation}     F(\\mathbf{x}) = \\begin{bmatrix} \\mathbf{H} \\mathbf{x}_1 \\\\ -\\mathbf{H}^\\top \\mathbf{x}_2 \\end{bmatrix} = \\begin{bmatrix} &amp; \\mathbf{H} \\\\ -\\mathbf{H}^\\top &amp; \\end{bmatrix} \\mathbf{x} \\end{equation} $$</p> <p>and $\\mathcal{S} = \\Delta_1 \\times \\Delta_2$</p>"},{"location":"examples/zero-sum-game-nb/#references","title":"References\u00b6","text":"<p>[1] Lemke, C. E., &amp; Howson, Jr, J. T. (1964). Equilibrium points of bimatrix games. Journal of the Society for industrial and Applied Mathematics, 12(2), 413-423.</p>"}]}