{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"monviso documentation","text":""},{"location":"#installation","title":"Installation","text":"<p>Install <code>monviso</code> directly from its GitHub repository using <code>pip</code>:</p> <pre><code>pip install git+https://github.com/nicomignoni/monviso.git@master\n</code></pre>"},{"location":"#getting-started","title":"Getting started","text":"<p>If you're already familiar with variational inequalities (VI), hop to the Quickstart for an overview on how to use <code>monviso</code>. Otherwise, the following provides an (extremely) essential introduction to VIs and to the nomenclature that will be used in the rest of the documentation. </p> <p>Given a vector mapping \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) and a scalar convex (possibly non-smooth) function \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\), solving a VI consists of solving the following </p> \\[\\begin{equation}     \\label{eq:vi_base}     \\text{find } \\mathbf{x}^* \\in \\mathbb{R}^n \\text{ such that } (\\mathbf{x} - \\mathbf{x}^*)^\\top \\mathbf{F}(\\mathbf{x}^*) - g(\\mathbf{x}) - g(\\mathbf{x}^*) \\geq 0, \\quad \\forall \\mathbf{x} \\in \\mathbb{R}^n. \\end{equation}\\] <p>It turns out that a lot of problems in optimal control, optimization, machine learning, game theory, finance, and much more, boil down to solving some instance of \\(\\eqref{eq:vi_base}\\). Such a problem is usually solved through iterative methods: one constructs an algorithm that produces a \\(\\mathbf{x}_k\\) at each iteration \\(k\\) such that \\(\\mathbf{x}_k \\to \\mathbf{x}^*\\) when \\(k \\to \\infty\\). </p> <p>What <code>monviso</code> does is providing a convenient way for accessing and using these iterative methods for solving an instance of \\(\\eqref{eq:vi_base}\\). The iterative methods that are currently implemented are listed in the API documentation. Since many of these algorithms rely on evaluating a proximal operator (or a projection step), <code>monviso</code> builds on top of <code>cvxpy</code>, a package for modelling and solving convex optimization problems.  </p>"},{"location":"#cite-as","title":"Cite as","text":"<p>If you used <code>monviso</code> in your project or research, please consider citing the related paper:</p> <pre><code>@inproceedings{mignoni2025monviso,\n  title={monviso: A Python Package for Solving Monotone Variational Inequalities},\n  author={Mignoni, Nicola and Baghbadorani, Reza Rahimi and Carli, Raffaele and Esfahani, Peyman Mohajerin and Dotoli, Mariagrazia and Grammatico, Sergio},\n  booktitle={2025 European Control Conference (ECC)},\n  year={2025}\n  organization={IEEE}\n}\n</code></pre>"},{"location":"#why-monviso","title":"Why \"<code>monviso</code>\"?","text":"<p>It stands for <code>mon</code>otone <code>v</code>ariational <code>i</code>nequalities <code>so</code>lutions. Initially, <code>so</code>- stood for \"solver\", but it was a bit on the nose. After all, <code>monviso</code> is just a collection of functions. Monviso is also a mountain in Italy. After a couple of iterations, the name in its current form was suggested by Sergio Grammatico.    </p>"},{"location":"api/","title":"API","text":""},{"location":"api/#the-vi-class","title":"The <code>VI</code> Class","text":"<p>The <code>VI</code> class defines the variational inequality. It is characterized by the vector mapping <code>F</code>and the <code>prox</code> operator. Both take and return a <code>np.ndarray</code>. <code>F</code> is a callable defined and passed directly by the user, while the <code>prox</code> operator can be defined in different ways:</p> <ul> <li>by passing the <code>analytical_prox</code> callable, which takes and returns a <code>np.ndarray</code></li> <li> <p>by defining a <code>y</code>, which is a <code>cp.Variable</code> and, optionally:</p> <ul> <li>a callable <code>g</code>, taking a <code>cp.Variable</code> as argument and retuning a scalar. If not define, it will default to <code>g = 0</code>. </li> <li>a constraint set <code>S</code>, with each element being a <code>cp.Constraint</code>. If not defined, it will default to <code>S = []</code>.</li> </ul> </li> </ul>"},{"location":"api/#monviso.VI","title":"monviso.VI","text":"<p>Attributes:</p> <ul> <li> <code>F</code>               (<code>callable</code>)           \u2013            <p>The VI vector mapping, i.e., \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\); a function transforming a <code>np.ndarray</code> into another <code>np.ndarray</code> of the same size.</p> </li> <li> <code>g</code>               (<code>(callable, optional)</code>)           \u2013            <p>The VI scalar mapping, i.e., \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\); a callable returning a <code>cvxpy.Expression</code></p> </li> <li> <code>y</code>               (<code>(Variable, optional)</code>)           \u2013            <p>The variable of the to be computed by the proximal operator, i.e., \\(\\mathbf{y}\\).    </p> </li> <li> <code>S</code>               (<code>list of cp.Constraints, optional</code>)           \u2013            <p>The constraints set, i.e., \\(\\mathcal{S} \\subseteq \\mathbb{R}^n\\); a list of <code>cvxpy.Constraint</code></p> </li> <li> <code>analytical_prox</code>               (<code>(Callable, optional)</code>)           \u2013            <p>The analytical user-defined function for the proximal operator.</p> </li> </ul>"},{"location":"api/#iterative-methods","title":"Iterative methods","text":"<p>The convention used for naming indexed terms is the following:</p> <ul> <li>Indexed terms' names are single letters</li> <li><code>xk</code> stands for \\(x_k\\)</li> <li><code>xkn</code> stands for \\(x_{k+n}\\) </li> <li><code>xnk</code> stands for \\(x_{k-n}\\)</li> </ul> <p>Therefore, as examples, <code>y0</code> is \\(y_0\\), <code>tk</code> is \\(t_k\\), <code>z1k</code> is \\(z_{k-1}\\), and <code>sk2</code> is \\(s_{k+2}\\). The vector corresponding to the decision variable of the VI is always denoted with \\(\\mathbf{x}\\); all other vectors that might be used and / returned are generically referred to as auxiliary points.</p>"},{"location":"api/#proximal-gradient","title":"Proximal gradient","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and an initial vector \\(\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate of the proximal gradient (PG) algorithm is <sup>1</sup>:</p> \\[ \\mathbf{x}_{k+1} = \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k - \\chi \\mathbf{F}(\\mathbf{x}_k)) \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. Convergence of PG is guaranteed for Lipschitz strongly monotone operators, with monotone constant \\(\\mu &gt; 0\\) and Lipschitz constants \\(L &lt; +\\infty\\), when \\(\\chi \\in (0, 2\\mu/L^2)\\).</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> </ul>"},{"location":"api/#monviso.VI.pg","title":"monviso.VI.pg","text":"<pre><code>pg(xk: ndarray, step_size: float, **kwargs)\n</code></pre>"},{"location":"api/#extragradient","title":"Extragradient","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and an initial vector \\(\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the \\(k\\)-th iterate of the extragradient algorithm (EG) is<sup>2</sup>:</p> \\[  \\begin{align*}     \\mathbf{y}_k &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k -          \\chi \\mathbf{F}(\\mathbf{x}_k)) \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{y}_k -          \\chi \\mathbf{F}(\\mathbf{x}_k)) \\end{align*} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of the EGD algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{L}\\right)\\).</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\). </li> </ul>"},{"location":"api/#monviso.VI.eg","title":"monviso.VI.eg","text":"<pre><code>eg(xk: ndarray, step_size: float, **kwargs)\n</code></pre>"},{"location":"api/#popovs-method","title":"Popov's method","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and an initial vectors \\(\\mathbf{x}_0,\\mathbf{y}_0 \\in \\mathbb{R}^n\\), the \\(k\\)-th iterate of Popov's Method (PM) is<sup>3</sup>:</p> \\[  \\begin{align*}     \\mathbf{y}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k - \\chi \\mathbf{F}(\\mathbf{y}_k)) \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{y}_{k+1} - \\chi \\mathbf{F}(\\mathbf{x}_k)) \\end{align*} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of PM is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{2L}\\right)\\).</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\). </li> <li><code>yk</code> (<code>ndarray</code>) \u2013 The current auxiliary point, corresponding to \\(\\mathbf{y}_k\\)</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> <li><code>yk1</code> (<code>ndarray</code>) \u2013 The next auxiliary point, corresponding to \\(\\mathbf{y}_{k+1}\\)</li> </ul>"},{"location":"api/#monviso.VI.popov","title":"monviso.VI.popov","text":"<pre><code>popov(xk: ndarray, yk: ndarray, step_size: float, **kwargs)\n</code></pre>"},{"location":"api/#forward-backward-forward","title":"Forward-backward-forward","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and an initial vector \\(\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the \\(k\\)-th iterate of Forward-Backward-Forward (FBF) algorithm is<sup>4</sup>:</p> \\[  \\begin{align*}     \\mathbf{y}_k &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k - \\chi \\mathbf{F}(\\mathbf{x}_k)) \\\\     \\mathbf{x}_{k+1} &amp;= \\mathbf{y}_k - \\chi \\mathbf{F}(\\mathbf{y}_k) + \\chi \\mathbf{F}(\\mathbf{x}_k) \\end{align*} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of the FBF algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{L}\\right)\\).</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> </ul>"},{"location":"api/#monviso.VI.fbf","title":"monviso.VI.fbf","text":"<pre><code>fbf(xk: ndarray, step_size: float, **kwargs)\n</code></pre>"},{"location":"api/#forward-reflected-backward","title":"Forward-reflected-backward","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors \\(\\mathbf{x}_1,\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate of the Forward-Reflected-Backward (FRB) is the following<sup>5</sup>:</p> \\[ \\mathbf{x}_{k+1} = \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k - \\chi (2\\mathbf{F}(\\mathbf{x}_k) + \\mathbf{F}(\\mathbf{x}_{k-1}))) \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of the FRB algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{2L}\\right)\\).</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>x1k</code> (<code>ndarray</code>) \u2013 The previous point, corresponding to \\(\\mathbf{x}_{k-1}\\).</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> </ul>"},{"location":"api/#monviso.VI.frb","title":"monviso.VI.frb","text":"<pre><code>frb(xk: ndarray, x1k: ndarray, step_size: float, **kwargs)\n</code></pre>"},{"location":"api/#projected-reflected-gradient","title":"Projected reflected gradient","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors \\(\\mathbf{x}_1,\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate of the projected reflected gradient (PRG) is the following <sup>6</sup>:</p> \\[ \\mathbf{x}_{k+1} = \\text{prox}_{g,\\mathcal{S}}(\\mathbf{x}_k - \\chi \\mathbf{F}(2\\mathbf{x}_k - \\mathbf{x}_{k-1})) \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of PRG algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constants \\(L &lt; +\\infty\\), when \\(\\chi \\in (0,(\\sqrt{2} - 1)/L)\\). Differently from the EGD iteration, the PRGD has the advantage of requiring a single proximal operator evaluation.</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>x1k</code> (<code>ndarray</code>) \u2013 The previous point, corresponding to \\(\\mathbf{x}_{k-1}\\).</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> </ul>"},{"location":"api/#monviso.VI.prg","title":"monviso.VI.prg","text":"<pre><code>prg(xk: ndarray, x1k: ndarray, step_size: float, **kwargs)\n</code></pre>"},{"location":"api/#extra-anchored-gradient","title":"Extra anchored gradient","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and an initial vector \\(\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the \\(k\\)-th  iterate of extra anchored gradient (EAG) algorithm is <sup>7</sup>:</p> \\[ \\begin{align*}     \\mathbf{y}_k &amp;= \\text{prox}_{g,\\mathcal{S}}\\left(\\mathbf{x}_k -          \\chi \\mathbf{F}(\\mathbf{x}_k) + \\frac{1}{k+1}(\\mathbf{x}_0 -          \\mathbf{x}_k)\\right) \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}\\left(\\mathbf{x}_k -          \\chi \\mathbf{F}(\\mathbf{y}_k) + \\frac{1}{k+1}(\\mathbf{x}_0 -          \\mathbf{x}_k)\\right) \\end{align*} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex  (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of the EAG algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{\\sqrt{3}L} \\right)\\).</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>x0</code> (<code>ndarray</code>) \u2013 The initial point, corresponding to \\(\\mathbf{x}_0\\).</li> <li>{{ k }}</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> </ul>"},{"location":"api/#monviso.VI.eag","title":"monviso.VI.eag","text":"<pre><code>eag(\n    xk: ndarray,\n    x0: ndarray,\n    k: int,\n    step_size: float,\n    **kwargs\n)\n</code></pre>"},{"location":"api/#accelerated-reflected-gradient","title":"Accelerated reflected gradient","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors \\(\\mathbf{x}_1,\\mathbf{x}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate of the accelerated reflected gradient (ARG) is the following<sup>8</sup>:</p> \\[ \\begin{align*}     \\mathbf{y}_k &amp;= 2\\mathbf{x}_k - \\mathbf{x}_{k-1} + \\frac{1}{k+1}     (\\mathbf{x}_0 - \\mathbf{x}_k) - \\frac{1}{k}(\\mathbf{x}_k -      \\mathbf{x}_{k-1}) \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}\\left(\\mathbf{x}_k -          \\chi \\mathbf{F}(\\mathbf{y}_k) + \\frac{1}{k+1}(\\mathbf{x}_0 -          \\mathbf{x}_k)\\right) \\end{align*} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of the ARG algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{12L}\\right)\\).</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\). </li> <li><code>x1k</code> (<code>ndarray</code>) \u2013 The previous point, corresponding to \\(\\mathbf{x}_{k-1}\\).</li> <li><code>x0</code> (<code>ndarray</code>) \u2013 The initial point, corresponding to \\(\\mathbf{x}_0\\).</li> <li>{{ k }}</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> </ul>"},{"location":"api/#monviso.VI.arg","title":"monviso.VI.arg","text":"<pre><code>arg(\n    xk: ndarray,\n    x1k: ndarray,\n    x0: ndarray,\n    k: int,\n    step_size: float,\n    **kwargs\n)\n</code></pre>"},{"location":"api/#explicit-fast-optimistic-gradient-descent-ascent","title":"(Explicit) fast optimistic gradient descent-ascent","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors \\(\\mathbf{x}_1,\\mathbf{x}_0,\\mathbf{y}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate of the explicit fast OGDA (FOGDA) is the following <sup>9</sup>:</p> \\[ \\begin{align*}     \\mathbf{y}_k &amp;= \\mathbf{x}_k + \\frac{k}{k+\\alpha}(\\mathbf{x}_k -          \\mathbf{x}_{k-1}) - \\chi \\frac{\\alpha}{k+\\alpha}         \\mathbf{F}(\\mathbf{y}_{k-1}) \\\\     \\mathbf{x}_{k+1} &amp;= \\mathbf{y}_k - \\chi \\frac{2k+\\alpha}         {k+\\alpha} (\\mathbf{F}(\\mathbf{y}_k) - \\mathbf{F}(\\mathbf{y}_{k-1})) \\end{align*} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of the ARG algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{4L}\\right)\\) and \\(\\alpha &gt; 2\\).</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>x1k</code> (<code>ndarray</code>) \u2013 The previous point, corresponding to \\(\\mathbf{x}_{k-1}\\).</li> <li><code>y1k</code> (<code>ndarray</code>) \u2013 The previous point, corresponding to \\(\\mathbf{y}_{k-1}\\)</li> <li>{{ k }}</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>alpha</code> (<code>float</code>, optional) \u2013 The auxiliary parameter, corresponding to \\(\\alpha\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> <li><code>yk</code> (<code>ndarray</code>) \u2013 The current auxiliary point, corresponding to \\(\\mathbf{y}_k\\)</li> </ul>"},{"location":"api/#monviso.VI.fogda","title":"monviso.VI.fogda","text":"<pre><code>fogda(\n    xk: ndarray,\n    x1k: ndarray,\n    y1k: ndarray,\n    k: int,\n    step_size: float,\n    alpha: float = 2.1,\n)\n</code></pre>"},{"location":"api/#constrained-fast-optimistic-gradient-descent-ascent","title":"Constrained fast optimistic gradient descent-ascent","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors \\(\\mathbf{x}_1 \\in \\mathcal{S}\\), \\(\\mathbf{z}_1 \\in N_{\\mathcal{S}}(\\mathbf{x}_1)\\), \\(\\mathbf{x}_0,\\mathbf{y}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate of Constrained Fast Optimistic Gradient Descent Ascent (CFOGDA) is the following<sup>10</sup>:</p> \\[  \\begin{align*}     \\mathbf{y}_k &amp;= \\mathbf{x}_k + \\frac{k}{k+\\alpha}(\\mathbf{x}_k - \\mathbf{x}_{k-1}) - \\chi \\frac{\\alpha}{k+\\alpha}(\\mathbf{F}(\\mathbf{y}_{k-1}) + \\mathbf{z}_k) \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}\\left(\\mathbf{y}_k - \\chi\\left(1 + \\frac{k}{k+\\alpha}\\right)(\\mathbf{F}(\\mathbf{y}_k) - \\mathbf{F}(\\mathbf{y}_{k-1}) - \\zeta_k)\\right) \\\\     \\mathbf{z}_{k+1} &amp;= \\frac{k+\\alpha}{\\chi (2k+\\alpha)}( \\mathbf{y}_k - \\mathbf{x}_{k+1}) - (\\mathbf{F}(\\mathbf{y}_k) - \\mathbf{F}(\\mathbf{y}_{k-1}) - \\zeta_k) \\end{align*} \\] <p>where \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) is a scalar convex (possibly non-smooth) function, while \\(\\mathbf{F} : \\mathbb{R}^n \\to \\mathbb{R}^n\\) is the VI mapping. The convergence of the CFOGDA algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constant \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{1}{4L}\\right)\\) and \\(\\alpha &gt; 2\\).</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>x1k</code> (<code>ndarray</code>) \u2013 The previous point, corresponding to \\(\\mathbf{x}_{k-1}\\). </li> <li><code>y1k</code> (<code>ndarray</code>) \u2013 The previous point, corresponding to \\(\\mathbf{y}_{k-1}\\)</li> <li><code>zk</code> (<code>ndarray</code>) \u2013 The current auxiliary point, corresponding to \\(\\mathbf{z}_k\\)</li> <li>{{ k }}</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>alpha</code> (<code>float</code>, optional) \u2013 The auxiliary parameter, corresponding to \\(\\alpha\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> <li><code>yk</code> (<code>ndarray</code>) \u2013 The current auxiliary point, corresponding to \\(\\mathbf{y}_k\\)</li> <li><code>zk1</code> (<code>ndarray</code>) \u2013 The next auxiliary point, corresponding to \\(\\mathbf{z}_{k+1}\\)</li> </ul>"},{"location":"api/#monviso.VI.cfogda","title":"monviso.VI.cfogda","text":"<pre><code>cfogda(\n    xk: ndarray,\n    x1k: ndarray,\n    y1k: ndarray,\n    zk: ndarray,\n    k: int,\n    step_size: float,\n    alpha: float = 2.1,\n    **kwargs\n)\n</code></pre>"},{"location":"api/#golden-ratio-algorithm","title":"Golden ratio algorithm","text":"<p>Given a constant step-size \\(\\chi &gt; 0\\) and initial vectors \\(\\mathbf{x}_0,\\mathbf{y}_0 \\in \\mathbb{R}^n\\), the basic \\(k\\)-th iterate the golden ratio algorithm (GRAAL) is the following <sup>11</sup>:</p> \\[  \\begin{align*}     \\mathbf{y}_{k+1} &amp;= \\frac{(\\phi - 1)\\mathbf{x}_k + \\phi\\mathbf{y}_k}{\\phi} \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{y}_{k+1} - \\chi \\mathbf{F}(\\mathbf{x}_k)) \\end{align*} \\] <p>The convergence of GRAAL algorithm is guaranteed for Lipschitz monotone operators, with Lipschitz constants \\(L &lt; +\\infty\\), when \\(\\chi \\in \\left(0,\\frac{\\varphi}{2L}\\right]\\) and \\(\\phi \\in (1,\\varphi]\\), where \\(\\varphi = \\frac{1+\\sqrt{5}}{2}\\) is the golden ratio.</p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>yk</code> (<code>ndarray</code>) \u2013 The current auxiliary point, corresponding to \\(\\mathbf{y}_k\\)</li> <li><code>step_size</code> (<code>float</code>) \u2013 The steps size value, corresponding to \\(\\chi\\).</li> <li><code>phi</code> (<code>float</code>, optional) \u2013 The golden ratio step size, corresponding to \\(\\phi\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> <li><code>yk1</code> (<code>ndarray</code>) \u2013 The next auxiliary point, corresponding to \\(\\mathbf{y}_{k+1}\\)</li> </ul>"},{"location":"api/#monviso.VI.graal","title":"monviso.VI.graal","text":"<pre><code>graal(\n    xk: ndarray,\n    yk: ndarray,\n    step_size: float,\n    phi: float = GOLDEN_RATIO,\n    **kwargs\n)\n</code></pre>"},{"location":"api/#adaptive-golden-ratio-algorithm","title":"Adaptive golden ratio algorithm","text":"<p>The Adaptive Golden Ratio Algorithm (aGRAAL) algorithm is a variation of the Golden Ratio Algorithm (monviso.VI.graal), with adaptive step size.  Following <sup>12</sup>, let \\(\\theta_0 = 1\\), \\(\\rho = 1/\\phi + 1/\\phi^2\\), where \\(\\phi \\in (0,\\varphi]\\) and \\(\\varphi = \\frac{1+\\sqrt{5}}{2}\\) is the golden ratio.  Moreover, let \\(\\bar{\\chi} \\gg 0\\) be a constant (arbitrarily large) step-size.  Given the initial terms \\(\\mathbf{x}_0,\\mathbf{x}_1 \\in \\mathbb{R}^n\\), \\(\\mathbf{y}_0 = \\mathbf{x}_1\\), and \\(\\chi_0 &gt; 0\\), the \\(k\\)-th iterate for aGRAAL is the following:</p> \\[ \\begin{align*}  \\chi_k &amp;= \\min\\left\\{\\rho\\chi_{k-1},       \\frac{\\phi\\theta_k \\|\\mathbf{x}_k       -\\mathbf{x}_{k-1}\\|^2}{4\\chi_{k-1}\\|\\mathbf{F}(\\mathbf{x}_k)       -\\mathbf{F}(\\mathbf{x}_{k-1})\\|^2}, \\bar{\\chi}\\right\\} \\\\ \\mathbf{x}_{k+1}, \\mathbf{y}_{k+1} &amp;= \\texttt{graal}(\\mathbf{x}_k, \\mathbf{y}_k, \\chi_k, \\phi) \\\\ \\theta_{k+1} &amp;= \\phi\\frac{\\chi_k}{\\chi_{k-1}}  \\end{align*} \\] <p>The convergence guarantees discussed for GRAAL also hold for aGRAAL. </p> <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>x1k</code> (<code>ndarray</code>) \u2013 The previous point, corresponding to \\(\\mathbf{x}_{k-1}\\).</li> <li><code>yk</code> (<code>ndarray</code>) \u2013 The current auxiliary point, corresponding to \\(\\mathbf{y}_k\\)</li> <li><code>s1k</code> (<code>float</code>) \u2013 The previous step-size, corresponding to \\(s_{k-1}\\)</li> <li><code>tk</code> (<code>float</code>, optional) The current auxiliary coefficient, corresponding to \\(\\theta_k\\).</li> <li><code>step_size_large</code> (<code>float</code>) \u2013 A constant (arbitrarily) large value for the step size, corresponding to \\(\\bar{\\chi}\\).</li> <li><code>phi</code> (<code>float</code>, optional) \u2013 The golden ratio step size, corresponding to \\(\\phi\\).</li> <li><code>**cvxpy_solve_params</code> \u2013 The parameters for the <code>cvxpy.Problem.solve</code> method.</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\).</li> <li><code>yk1</code> (<code>ndarray</code>) \u2013 The next auxiliary point, corresponding to \\(\\mathbf{y}_{k+1}\\) </li> <li><code>sk</code> (<code>float</code>) \u2013 The current steps-size, corresponding to \\(s_k\\) </li> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next auxiliary coefficient, corresponding to \\(\\theta_{k+1}\\).</li> </ul>"},{"location":"api/#monviso.VI.agraal","title":"monviso.VI.agraal","text":"<pre><code>agraal(\n    xk: ndarray,\n    x1k: ndarray,\n    yk: ndarray,\n    s1k: float,\n    tk: float = 1,\n    step_size_large: float = 1000000.0,\n    phi: float = GOLDEN_RATIO,\n    **kwargs\n)\n</code></pre>"},{"location":"api/#hybrid-golden-ratio-algorithm-i","title":"Hybrid golden ratio algorithm I","text":"<p>The HGRAAL-1 algorithm<sup>13</sup> is a variation of the Adaptive Golden Ratio Algorithm (monviso.VI.agraal).  Let \\(\\theta_0 = 1\\), \\(\\rho = 1/\\phi + 1/\\phi^2\\), where \\(\\phi \\in (0,\\varphi]\\) and \\(\\varphi = \\frac{1+\\sqrt{5}}{2}\\) is the golden ratio.  The residual at point \\(\\mathbf{x}_k\\) is given by \\(J : \\mathbb{R}^n \\to \\mathbb{R}\\), defined as follows:</p> \\[ J(\\mathbf{x}_k) = \\|\\mathbf{x}_k - \\text{prox}_{g,\\mathcal{S}} (\\mathbf{x}_k - \\mathbf{F}(\\mathbf{x}_k))\\| \\] <p>Moreover, let \\(\\bar{\\chi} \\gg 0\\) be a constant (arbitrarily large) step-size.  Given the initial terms \\(\\mathbf{x}_0,\\mathbf{x}_1 \\in\\mathbb{R}^n\\), \\(\\mathbf{y}_0 = \\mathbf{x}_1\\), and \\(\\chi_0 &gt; 0\\), the \\(k\\)-th iterate for HGRAAL-1 is the following:</p> \\[  \\begin{align*}     \\chi_k &amp;= \\min\\left\\{\\rho\\chi_{k-1},         \\frac{\\phi\\theta_k \\|\\mathbf{x}_k         -\\mathbf{x}_{k-1}\\|^2}{4\\chi_{k-1}\\|\\mathbf{F}(\\mathbf{x}_k)         -\\mathbf{F}(\\mathbf{x}_{k-1})\\|^2}, \\bar{\\chi}\\right\\} \\\\     c_k &amp;= \\left(\\langle J(\\mathbf{x}_k) - J(\\mathbf{x}_{k-1}) &gt; 0 \\rangle          \\text{ and } \\langle f_k \\rangle \\right)          \\text{ or } \\left\\langle \\min\\{J(\\mathbf{x}_{k-1}), J(\\mathbf{x}_k)\\} &lt;          J(\\mathbf{x}_k) + \\frac{1}{\\bar{k}} \\right\\rangle \\\\     f_k &amp;= \\text{not $\\langle c_k \\rangle$} \\\\     \\bar{k} &amp;= \\begin{cases} \\bar{k}+1 &amp; \\text{if $c_k$ is true} \\\\          \\bar{k} &amp; \\text{otherwise} \\end{cases} \\\\     \\mathbf{y}_{k+1} &amp;=          \\begin{cases}             \\dfrac{(\\phi - 1)\\mathbf{x}_k + \\phi\\mathbf{y}_k}{\\phi} &amp;              \\text{if $c_k$ is true} \\\\             \\mathbf{x}_k &amp; \\text{otherwise}         \\end{cases} \\\\     \\mathbf{x}_{k+1} &amp;= \\text{prox}_{g,\\mathcal{S}}(\\mathbf{y}_{k+1} - \\chi_k          \\mathbf{F}(\\mathbf{x}_k)) \\\\     \\theta_{k+1} &amp;= \\phi\\frac{\\chi_k}{\\chi_{k-1}}  \\end{align*} \\] <p>Parameters:</p> <ul> <li><code>xk</code> (<code>ndarray</code>) \u2013 The current point, corresponding to \\(\\mathbf{x}_k\\).</li> <li><code>x1k</code> (<code>ndarray</code>) \u2013 The previous point, corresponding to \\(\\mathbf{x}_{k-1}\\).</li> <li><code>yk</code> (<code>ndarray</code>) \u2013 The current auxiliary point, corresponding to \\(\\mathbf{y}_k\\)</li> <li><code>s1k</code> (<code>float</code>) \u2013 The previous step-size, corresponding to \\(s_{k-1}\\)</li> <li><code>tk</code> (<code>float</code>, optional) The current auxiliary coefficient, corresponding to \\(\\theta_k\\).</li> <li><code>ck</code> (<code>int</code>) \u2013 The current counting parameter, corresponding to \\(c_k\\)</li> <li><code>step_size_large</code> (<code>float</code>) \u2013 A constant (arbitrarily) large value for the step size, corresponding to \\(\\bar{\\chi}\\).</li> <li><code>phi</code> (<code>float</code>, optional) \u2013 The golden ratio step size, corresponding to \\(\\phi\\).</li> </ul> <p>Returns:</p> <ul> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next point, corresponding to \\(\\mathbf{x}_{k+1}\\). </li> <li><code>yk1</code> (<code>ndarray</code>) \u2013 The next auxiliary point, corresponding to \\(\\mathbf{y}_{k+1}\\)</li> <li><code>sk</code> (<code>float</code>) \u2013 The current steps-size, corresponding to \\(s_k\\)</li> <li><code>xk1</code> (<code>ndarray</code>) \u2013 The next auxiliary coefficient, corresponding to \\(\\theta_{k+1}\\).</li> <li><code>ck1</code> (<code>int</code>) \u2013 The next counting parameter, corresponding to \\(c_{k+1}\\)</li> </ul> <ol> <li> <p>Nemirovskij, A. S., &amp; Yudin, D. B. (1983). Problem complexity and method efficiency in optimization.\u00a0\u21a9</p> </li> <li> <p>Korpelevich, G. M. (1976). The extragradient method for finding     saddle points and other problems. Matecon, 12, 747-756.\u00a0\u21a9</p> </li> <li> <p>Popov, L.D. A modification of the Arrow-Hurwicz method for search of saddle points. Mathematical Notes of the Academy of Sciences of the USSR 28, 845\u2013848 (1980)\u00a0\u21a9</p> </li> <li> <p>Tseng, P. (2000). A modified forward-backward splitting method for maximal monotone mappings. SIAM Journal on Control and Optimization, 38(2), 431-446.\u00a0\u21a9</p> </li> <li> <p>Malitsky, Y., &amp; Tam, M. K. (2020). A forward-backward splitting method for monotone inclusions without cocoercivity. SIAM Journal on Optimization, 30(2), 1451-1472.\u00a0\u21a9</p> </li> <li> <p>Malitsky, Y. (2015). Projected reflected gradient methods for monotone variational inequalities. SIAM Journal on Optimization, 25(1), 502-520.\u00a0\u21a9</p> </li> <li> <p>Yoon, T., &amp; Ryu, E. K. (2021, July). Accelerated Algorithms for Smooth Convex-Concave Minimax Problems with O (1/k^ 2) Rate on Squared Gradient Norm. In International Conference on Machine Learning (pp. 12098-12109). PMLR.\u00a0\u21a9</p> </li> <li> <p>Cai, Y., &amp; Zheng, W. (2022). Accelerated single-call methods for constrained min-max optimization. arXiv preprint arXiv:2210.03096.\u00a0\u21a9</p> </li> <li> <p>Bo\u0163, R. I., Csetnek, E. R., &amp; Nguyen, D. K. (2023). Fast Optimistic Gradient Descent Ascent (OGDA) method in continuous and discrete time. Foundations of Computational Mathematics, 1-60.\u00a0\u21a9</p> </li> <li> <p>Sedlmayer, M., Nguyen, D. K., &amp; Bot, R. I. (2023, July). A fast optimistic method for monotone variational inequalities. In International Conference on Machine Learning (pp. 30406-30438). PMLR.\u00a0\u21a9</p> </li> <li> <p>Malitsky, Y. (2020). Golden ratio algorithms for variational inequalities. Mathematical Programming, 184(1), 383-410.\u00a0\u21a9</p> </li> <li> <p>Malitsky, Y. (2020). Golden ratio algorithms for variational inequalities. Mathematical Programming, 184(1), 383-410.\u00a0\u21a9</p> </li> <li> <p>Rahimi Baghbadorani, R., Mohajerin Esfahani, P., &amp; Grammatico, S. (2024). A hybrid algorithm for monotone variational inequalities.  (Manuscript submitted for publication).\u00a0\u21a9</p> </li> </ol>"},{"location":"api/#monviso.VI.hgraal_1","title":"monviso.VI.hgraal_1","text":"<pre><code>hgraal_1(\n    xk: ndarray,\n    x1k: ndarray,\n    yk: ndarray,\n    s1k: float,\n    tk: float = 1,\n    ck: int = 1,\n    phi: float = GOLDEN_RATIO,\n    step_size_large: float = 1000000.0,\n    **kwargs\n)\n</code></pre>"},{"location":"examples/feasibility-problem-nb/","title":"Feasibility problem","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom monviso import VI\n\nnp.random.seed(2024)\n\n# Problem data\nn, M = 10, 50\nc = np.random.normal(0, 100, size=(M, n))\nr = 1 - np.linalg.norm(c, axis=0)\n</pre> import numpy as np import cvxpy as cp import matplotlib.pyplot as plt  from monviso import VI  np.random.seed(2024)  # Problem data n, M = 10, 50 c = np.random.normal(0, 100, size=(M, n)) r = 1 - np.linalg.norm(c, axis=0) <p>Then, let's create the projection operator $\\mathbf{P}(\\cdot)$ and the VI mapping $\\mathbf{F}(\\cdot)$.</p> In\u00a0[2]: Copied! <pre>P = lambda x: np.where(\n    np.linalg.norm(x - c) &gt; r, \n    r * (x - c) / np.linalg.norm(x - c, axis=0), \n    x\n)\nT = lambda x: P(x).mean(axis=0)\nF = lambda x: x - T(x)\n</pre> P = lambda x: np.where(     np.linalg.norm(x - c) &gt; r,      r * (x - c) / np.linalg.norm(x - c, axis=0),      x ) T = lambda x: P(x).mean(axis=0) F = lambda x: x - T(x) <p>Finally, let's instantiate the VI with and initial solution. Note that $\\mathcal{X} = \\mathbb{R}^n$, so the proximal operator is the identity itself.</p> In\u00a0[3]: Copied! <pre>fp = VI(F)\nx = np.random.rand(n)\n</pre> fp = VI(F) x = np.random.rand(n) <p>The projection $\\mathbf{P}(\\cdot)$ is merely monotone, so $\\mathbf{F}(\\cdot)$ is merely monotone as well. As examples, we can use the forward-reflected-backward (<code>frb</code>), the projected reflected gradient (<code>prg</code>), and the extra anchored gradient (<code>eag</code>).</p> In\u00a0[22]: Copied! <pre>max_iter = 200\nstep_size = 0.01\n\nresiduals_frb = np.zeros(max_iter)\nresiduals_prg = np.zeros(max_iter)\nresiduals_eag = np.zeros(max_iter)\n\n# Copy the same initial solution for the three methods\nxk_frb, x1k_frb = np.copy(x), np.copy(x)\nxk_prg, x1k_prg = np.copy(x), np.copy(x)\nxk_eag, x0_eag = np.copy(x), np.copy(x)\n\nfor k in range(max_iter):\n    # Forward-reflected-backward\n    xk1_frb = fp.frb(xk_frb, x1k_frb, step_size)\n    residuals_frb[k] = np.linalg.norm(xk1_frb - xk_frb)\n    xk_frb, x1k_frb = xk1_frb, xk_frb\n\n    # Projected reflected gradient\n    xk1_prg = fp.prg(xk_prg, x1k_prg, step_size)\n    residuals_prg[k] = np.linalg.norm(xk1_prg - xk_prg)\n    xk_prg = xk1_prg\n\n    # Extra anchored gradient\n    xk1_eag = fp.eag(xk_eag, x0_eag, k, step_size)\n    residuals_eag[k] = np.linalg.norm(xk1_eag - xk_eag)\n    xk_eag = xk1_eag\n</pre> max_iter = 200 step_size = 0.01  residuals_frb = np.zeros(max_iter) residuals_prg = np.zeros(max_iter) residuals_eag = np.zeros(max_iter)  # Copy the same initial solution for the three methods xk_frb, x1k_frb = np.copy(x), np.copy(x) xk_prg, x1k_prg = np.copy(x), np.copy(x) xk_eag, x0_eag = np.copy(x), np.copy(x)  for k in range(max_iter):     # Forward-reflected-backward     xk1_frb = fp.frb(xk_frb, x1k_frb, step_size)     residuals_frb[k] = np.linalg.norm(xk1_frb - xk_frb)     xk_frb, x1k_frb = xk1_frb, xk_frb      # Projected reflected gradient     xk1_prg = fp.prg(xk_prg, x1k_prg, step_size)     residuals_prg[k] = np.linalg.norm(xk1_prg - xk_prg)     xk_prg = xk1_prg      # Extra anchored gradient     xk1_eag = fp.eag(xk_eag, x0_eag, k, step_size)     residuals_eag[k] = np.linalg.norm(xk1_eag - xk_eag)     xk_eag = xk1_eag <p>The API docs have some detail on the convention for naming iterative steps. Let's check out the residuals for each method.</p> In\u00a0[23]: Copied! <pre>fig, ax = plt.subplots()\n\nax.plot(residuals_frb, label=\"Forward reflected backward\")\nax.plot(residuals_prg, label=\"Forward-backward-forward\")\nax.plot(residuals_eag, label=\"Extra anchored gradient\")\n\nax.grid(True, alpha=0.2)\nax.set_xlabel(\"Iterations ($k$)\")\nax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\")\nplt.legend()\nplt.show()\n</pre> fig, ax = plt.subplots()  ax.plot(residuals_frb, label=\"Forward reflected backward\") ax.plot(residuals_prg, label=\"Forward-backward-forward\") ax.plot(residuals_eag, label=\"Extra anchored gradient\")  ax.grid(True, alpha=0.2) ax.set_xlabel(\"Iterations ($k$)\") ax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\") plt.legend() plt.show()"},{"location":"examples/feasibility-problem-nb/#feasibility-problem","title":"Feasibility problem\u00b6","text":"<p>Let us consider $M$ balls in $\\mathbb{R}^n$, where the $i$-th ball of radius $r_i &gt; 0$ centered in $\\mathbf{c}_i \\in \\mathbb{R}^n$ is given by $\\mathcal{B}_i(\\mathbf{c}_i, r_i) \\subset \\mathbb{R}^n$. We are interested in finding a point belonging to their intersection, i.e., we want to solve the following</p> <p>$$ \\begin{equation}      \\label{eq:intersection}     \\text{find} \\ \\mathbf{x} \\ \\text{subject to} \\ \\mathbf{x} \\in \\bigcap_{i = 1}^M \\mathcal{B}_i(\\mathbf{c}_i, r_i) \\end{equation} $$</p> <p>It is straightforward to verify that the projection of a point onto $\\mathcal{B}_i(\\mathbf{c}_i,r_i)$ is evaluated as</p> <p>$$ \\begin{equation}     \\label{eq:projection}     \\mathbf{P}_i(\\mathbf{x}) :=      \\text{proj}_{\\mathcal{B}_i(\\mathbf{c}_i,r_i)}(\\mathbf{x}) =      \\begin{cases}         \\displaystyle r_i\\frac{\\mathbf{x} -          \\mathbf{c}_i}{\\|\\mathbf{x} - \\mathbf{c}_i\\|} &amp; \\text{if} \\ \\|\\mathbf{x} - \\mathbf{c}_i\\| &gt; r_i \\\\         \\mathbf{x} &amp; \\text{otherwise}     \\end{cases} \\end{equation} $$</p> <p>Due to the non-expansiveness of the projection in $\\eqref{eq:projection}$, one can find a solution for $\\eqref{eq:intersection}$ as the fixed point of the following iterate</p> <p>$$ \\begin{equation}     \\label{eq:krasnoselskii-mann}     \\mathbf{x}_{k+1} = \\mathbf{T}(\\mathbf{x}_k) = \\frac{1}{M}\\sum_{i = 1}^M\\mathbf{P}_i(\\mathbf{x}_k)  \\end{equation} $$</p> <p>which result from the well-known Krasnoselskii-Mann iterate. By letting $\\mathbf{F} = \\mathbf{I} - \\mathbf{T}$, where $\\mathbf{I}$ denotes the identity operator, the fixed point for $\\eqref{eq:krasnoselskii-mann}$ can be treated as the canonical VI [1].</p> <p>Let's start by creating the problems' data.</p>"},{"location":"examples/feasibility-problem-nb/#references","title":"References\u00b6","text":"<p>[1] Bauschke, H. H., &amp; Borwein, J. M. (1996). On projection algorithms for solving convex feasibility problems. SIAM review, 38(3), 367-426.</p>"},{"location":"examples/linear-complementarity-nb/","title":"Linear Complementarity Problem","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport cvxpy as cp\n\nfrom utils import *\nfrom monviso import VI\n\nnp.random.seed(2024)\n\nn = 5\nq = np.random.uniform(-10, 10, n)\nM = random_positive_definite_matrix(-10, 10, n)\n</pre> import numpy as np import cvxpy as cp  from utils import * from monviso import VI  np.random.seed(2024)  n = 5 q = np.random.uniform(-10, 10, n) M = random_positive_definite_matrix(-10, 10, n) <p>Then, let's define the mapping $\\mathbf{F}(\\cdot)$ and te</p> In\u00a0[\u00a0]: Copied! <pre>F = lambda x: - q - M @ x\n\ny = cp.Variable(n)\nS = [x &gt;= 0]\n</pre> F = lambda x: - q - M @ x  y = cp.Variable(n) S = [x &gt;= 0] In\u00a0[\u00a0]: Copied! <pre>L = np.linalg.norm(M, 2)\nmu = np.linals.eigvals(M).min()\n</pre> L = np.linalg.norm(M, 2) mu = np.linals.eigvals(M).min() In\u00a0[1]: Copied! <pre># Define the VI and the initial(s) points\nlcp = VI(n, F, S=S)\nx0 = []\nx = cp.Variable(n)\nfor _ in range(2):\n    prob = cp.Problem(\n        cp.Minimize(np.random.rand(n) @ x),\n        constraints=[constraint(x) for constraint in S],\n    ).solve()\n    x0.append(x.value)\n\n# Solve the VI using the available algorithms\nmax_iter = 200\nfor algorithm, params in cases(x0, L, excluded={\"fogda\", \"cfogda\"}).items():\n    print(f\"Using: {algorithm}\")\n    sol = lcp.solution(\n        algorithm,\n        params,\n        max_iter,\n        log_path=f\"logs/linear-complementarity/{algorithm}.log\",\n    )\n\nplot_results(\n    \"logs/linear-complementarity\",\n    \"figs/linear-complementarity.pdf\",\n    r\"$\\|\\mathbf{x}_k \\! - \\! \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k \\! - \\! F(\\mathbf{x}_k))\\|$\",\n)\n</pre>     # Define the VI and the initial(s) points lcp = VI(n, F, S=S) x0 = [] x = cp.Variable(n) for _ in range(2):     prob = cp.Problem(         cp.Minimize(np.random.rand(n) @ x),         constraints=[constraint(x) for constraint in S],     ).solve()     x0.append(x.value)  # Solve the VI using the available algorithms max_iter = 200 for algorithm, params in cases(x0, L, excluded={\"fogda\", \"cfogda\"}).items():     print(f\"Using: {algorithm}\")     sol = lcp.solution(         algorithm,         params,         max_iter,         log_path=f\"logs/linear-complementarity/{algorithm}.log\",     )  plot_results(     \"logs/linear-complementarity\",     \"figs/linear-complementarity.pdf\",     r\"$\\|\\mathbf{x}_k \\! - \\! \\text{proj}_{\\mathcal{S}}(\\mathbf{x}_k \\! - \\! F(\\mathbf{x}_k))\\|$\", ) <pre>Using: pg\nUsing: eg\nUsing: popov\nUsing: fbf\nUsing: frb\nUsing: prg\nUsing: eag\nUsing: arg\nUsing: graal\nUsing: agraal\nUsing: hgraal_1\nUsing: hgraal_2\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/linear-complementarity-nb/#linear-complementarity-problem","title":"Linear Complementarity Problem\u00b6","text":"<p>A common problem that can be cast to a VI is the linear complementarity problem [1]: given $\\mathbf{q} \\in \\mathbb{R}^n$ and $\\mathbf{0} \\prec \\mathbf{M} \\in \\mathbb{R}^{n \\times n}$, one want to solve the following</p> <p>$$ \\begin{equation}     \\label{eq:complementarity}     \\text{find $\\mathbf{x} \\in \\mathbb{R}^n_{\\geq 0}$ s.t. $\\mathbf{y} = \\mathbf{M}\\mathbf{x} + \\mathbf{q}$, $\\mathbf{y}^\\top \\mathbf{x} = 0$} \\end{equation} $$</p> <p>By setting $F(\\mathbf{x}) = - \\mathbf{M}\\mathbf{x} - \\mathbf{q}$ and $\\mathcal{S} = \\mathbb{R}_{\\geq 0}$ it can be readily verified that each solution for $(\\mathbf{x} - \\mathbf{x}^*)^\\top F(\\mathbf{x}^*) \\geq 0$ is also a solution for $\\eqref{eq:complementarity}$.</p> <p>Let's start by creating some data</p>"},{"location":"examples/linear-complementarity-nb/#references","title":"References\u00b6","text":"<p>[1] Harker, P. T., &amp; Pang, J. S. (1990). For the linear complementarity problem. Lectures in Applied Mathematics, 26, 265-284.</p>"},{"location":"examples/linear-quadratic-game-nb/","title":"Linear-Quadratic Dynamic Game","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport scipy as sp\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom monviso import VI\n\nnp.random.rand(2024)\n\nmake_positive_semidefinite = lambda X: X * X.T\n\nn, m, N, T = 13, 4, 5, 3\n\nA = np.random.rand(n, n)\nB = [np.random.rand(n, m) for _ in range(N)]\nQ = [make_positive_semidefinite(np.random.rand(n, n)) for _ in range(N)]\nR = [make_positive_semidefinite(np.random.rand(m, m)) for _ in range(N)]\nP = np.random.rand(n, n)\nQ_bar = [sp.linalg.block_diag(np.kron(np.eye(T - 1), Q[i]), P) for i in range(N)]\nG = [\n    np.kron(np.eye(T), B[i])\n    + np.kron(\n        np.eye(1,T),\n        np.vstack([np.linalg.matrix_power(A, t) @ B[i] for t in range(T)]),\n    )\n    for i in range(N)\n]\nH = np.vstack([np.linalg.matrix_power(A, t) for t in range(1, T + 1)])\nx0 = np.random.rand(n)\n</pre> import numpy as np import scipy as sp import cvxpy as cp import matplotlib.pyplot as plt  from monviso import VI  np.random.rand(2024)  make_positive_semidefinite = lambda X: X * X.T  n, m, N, T = 13, 4, 5, 3  A = np.random.rand(n, n) B = [np.random.rand(n, m) for _ in range(N)] Q = [make_positive_semidefinite(np.random.rand(n, n)) for _ in range(N)] R = [make_positive_semidefinite(np.random.rand(m, m)) for _ in range(N)] P = np.random.rand(n, n) Q_bar = [sp.linalg.block_diag(np.kron(np.eye(T - 1), Q[i]), P) for i in range(N)] G = [     np.kron(np.eye(T), B[i])     + np.kron(         np.eye(1,T),         np.vstack([np.linalg.matrix_power(A, t) @ B[i] for t in range(T)]),     )     for i in range(N) ] H = np.vstack([np.linalg.matrix_power(A, t) for t in range(1, T + 1)]) x0 = np.random.rand(n) <p>Let's define the mapping $\\mathbf{F}(\\cdot)$ and the constraints set $\\mathcal{S}$</p> In\u00a0[3]: Copied! <pre># Define the mapping\nF1 = np.vstack([G[i].T @ Q_bar[i] for i in range(N)])\nF2 = np.hstack(G)\nF3 = sp.linalg.block_diag(*[np.kron(np.eye(T), R[i]) for i in range(N)])\nF = lambda u: F1 @ (F2 @ u + H @ x0) + F3 @ u\nL = np.linalg.norm(F1 @ F2 + F3, 2) + 1\n\n# Define a constraints set for the collective input\ny = cp.Variable(m * T * N)\nS = [y &gt;= 0]\n</pre> # Define the mapping F1 = np.vstack([G[i].T @ Q_bar[i] for i in range(N)]) F2 = np.hstack(G) F3 = sp.linalg.block_diag(*[np.kron(np.eye(T), R[i]) for i in range(N)]) F = lambda u: F1 @ (F2 @ u + H @ x0) + F3 @ u L = np.linalg.norm(F1 @ F2 + F3, 2) + 1  # Define a constraints set for the collective input y = cp.Variable(m * T * N) S = [y &gt;= 0] <p>Then, we can initialize the VI and an initial solution</p> In\u00a0[4]: Copied! <pre># Define the VI and the initial(s) points\nlqg = VI(F, y=y, S=S)\nu = np.random.rand(m * T * N)\n</pre> # Define the VI and the initial(s) points lqg = VI(F, y=y, S=S) u = np.random.rand(m * T * N) <p>We can use and compare different algorithms; e.g., fast optimistic gradient descent-ascent (<code>fogda</code>), golden ratio (<code>graal</code>), adaptive golden ratio (<code>agraal</code>)</p> In\u00a0[11]: Copied! <pre>GOLDEN_RATIO = 0.5 * (1 + np.sqrt(5))\n\nmax_iter = 50\n\nresiduals_fogda = np.zeros(max_iter)\nresiduals_graal = np.zeros(max_iter)\nresiduals_agraal = np.zeros(max_iter)\n\n# Copy the same initial solution for the three methods\nxk_fogda, x1k_fogda, y1k_fogda = np.copy(u), np.copy(u), np.copy(u)\nxk_graal, yk_graal = np.copy(u), np.copy(u)\nxk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = \\\n    np.copy(u), np.random.rand(m * T * N), np.copy(u), GOLDEN_RATIO/(2*L), 1\n\nfor k in range(max_iter):\n    # Fast Optimistic Gradient Descent Ascent \n    xk1_fogda, yk_fogda = lqg.fogda(xk_fogda, x1k_fogda, y1k_fogda, k, step_size=1/(4*L))\n    residuals_fogda[k] = np.linalg.norm(xk1_fogda - xk_fogda)\n    xk_fogda, x1k_fogda, y1k_fogda = xk1_fogda, xk_fogda, yk_fogda\n\n    # Golden ratio algorithm\n    xk1_graal, yk1_graal = lqg.graal(xk_graal, yk_graal, step_size=GOLDEN_RATIO/(2*L))\n    residuals_graal[k] = np.linalg.norm(xk1_graal - xk_graal)\n    xk_graal, yk_graal = xk1_graal, yk1_graal\n\n    # Adaptive golden ratio algorithm\n    xk1_agraal, yk1_agraal, sk_agraal, tk1_agraal = lqg.agraal(xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal)\n    residuals_agraal[k] = np.linalg.norm(xk1_agraal - xk_agraal)\n    xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = xk1_agraal, xk_agraal, yk1_agraal, sk_agraal, tk1_agraal\n</pre> GOLDEN_RATIO = 0.5 * (1 + np.sqrt(5))  max_iter = 50  residuals_fogda = np.zeros(max_iter) residuals_graal = np.zeros(max_iter) residuals_agraal = np.zeros(max_iter)  # Copy the same initial solution for the three methods xk_fogda, x1k_fogda, y1k_fogda = np.copy(u), np.copy(u), np.copy(u) xk_graal, yk_graal = np.copy(u), np.copy(u) xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = \\     np.copy(u), np.random.rand(m * T * N), np.copy(u), GOLDEN_RATIO/(2*L), 1  for k in range(max_iter):     # Fast Optimistic Gradient Descent Ascent      xk1_fogda, yk_fogda = lqg.fogda(xk_fogda, x1k_fogda, y1k_fogda, k, step_size=1/(4*L))     residuals_fogda[k] = np.linalg.norm(xk1_fogda - xk_fogda)     xk_fogda, x1k_fogda, y1k_fogda = xk1_fogda, xk_fogda, yk_fogda      # Golden ratio algorithm     xk1_graal, yk1_graal = lqg.graal(xk_graal, yk_graal, step_size=GOLDEN_RATIO/(2*L))     residuals_graal[k] = np.linalg.norm(xk1_graal - xk_graal)     xk_graal, yk_graal = xk1_graal, yk1_graal      # Adaptive golden ratio algorithm     xk1_agraal, yk1_agraal, sk_agraal, tk1_agraal = lqg.agraal(xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal)     residuals_agraal[k] = np.linalg.norm(xk1_agraal - xk_agraal)     xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = xk1_agraal, xk_agraal, yk1_agraal, sk_agraal, tk1_agraal <p>The API docs have some detail on the convention for naming iterative steps. Let's check out the residuals for each method.</p> In\u00a0[12]: Copied! <pre>fig, ax = plt.subplots()\n\nax.plot(residuals_fogda, label=\"Fast Optimistic Gradient Descent Ascent \")\nax.plot(residuals_graal, label=\"Golden ratio algorithm\")\nax.plot(residuals_agraal, label=\"Adaptive golden ratio algorithm\")\n\nax.grid(True, alpha=0.2)\nax.set_xlabel(\"Iterations ($k$)\")\nax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\")\nplt.legend()\nplt.show()\n</pre> fig, ax = plt.subplots()  ax.plot(residuals_fogda, label=\"Fast Optimistic Gradient Descent Ascent \") ax.plot(residuals_graal, label=\"Golden ratio algorithm\") ax.plot(residuals_agraal, label=\"Adaptive golden ratio algorithm\")  ax.grid(True, alpha=0.2) ax.set_xlabel(\"Iterations ($k$)\") ax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\") plt.legend() plt.show()"},{"location":"examples/linear-quadratic-game-nb/#linear-quadratic-dynamic-game","title":"Linear-Quadratic Dynamic Game\u00b6","text":"<p>As shown in Proposition 2 in [1], the receding horizon open-loop Nash equilibria (NE) can be reformulated as a non-symmetric variational inequality. Specifically, consider a set of agents $\\mathcal{N} = \\{1,\\dots,N\\}$ characterizing a state vector $\\mathbf{x}[t] \\in \\mathbb{R}^n$, whose (linear) dynamics is described as</p> <p>$$ \\begin{equation}     \\mathbf{x}[t+1] = \\mathbf{A}\\mathbf{x}[t] + \\sum_{i \\in \\mathcal{N}} \\mathbf{B}_i \\mathbf{u}_i[t] \\end{equation} $$</p> <p>for $t = 1, \\dots, T$. Each agent $i$ selfishly tries to choose $\\mathbf{u}_i[t] \\in \\mathbb{R}^m$ in order to minimize the following cost function</p> <p>$$ \\begin{equation}     J_i(\\mathbf{u}_i|\\mathbf{x}_0, \\mathbf{u}_{-i}) = \\frac{1}{2}\\sum_{t=0}^{T-1} \\|\\mathbf{x}[t|\\mathbf{x}_0, \\mathbf{u}]\\|^2_{\\mathbf{Q}_i} + \\|\\mathbf{u}_i[t] \\|^2_{\\mathbf{R}_i} \\end{equation} $$</p> <p>for some $0 \\preceq \\mathbf{Q}_i \\in \\mathbb{R}^{n \\times n}$ and $0 \\prec \\mathbf{R}_i \\in \\mathbb{R}^{m \\times m}$, with $\\mathbf{u}_{-i} = \\text{col}(\\mathbf{u}_j)_{j \\in \\mathcal{N}\\setminus \\{i\\}}$ and $\\mathbf{u}_j = \\text{col}(\\mathbf{u}_j[t])_{t=1}^T$. Moreover, $\\mathbf{u} = \\text{col}(\\mathbf{u}_i)_{i \\in \\mathcal{N}}$. The set of feasible inputs, for each agent $i \\in \\mathcal{N}$, is $\\mathcal{U}_i(\\mathbf{x}_0,\\mathbf{u}_{-i}) := \\{\\mathbf{u}_i \\in \\mathbb{R}^{mT} : \\mathbf{u}_i[t] \\in \\mathcal{U}_i(\\mathbf{u}_{-i}[t]), \\ \\forall t = 0,\\dots,T-1; \\ \\mathbf{x}[t|\\mathbf{x}_0, \\mathbf{u}] \\in \\mathcal{X}, \\ \\forall t = 1,\\dots,T\\}$, where $\\mathcal{X} \\in \\mathbb{R}^n$ is the set of feasible system states. Finally, $\\mathcal{U}(\\mathbf{x}_0) = \\{\\mathbf{u} \\in \\mathbb{R}^{mTN}: \\mathbf{u}_i \\in \\mathcal{U}(\\mathbf{x}_0,\\mathbf{u}_{-i}), \\ \\forall i \\in \\mathcal{N}\\}$. Following Definition 1 in [1], the sequence of input $\\mathbf{u}^*_i \\in \\mathcal{U}_i(\\mathbf{x}_0,\\mathbf{u}_{-i})$, for all $i \\in \\mathcal{N}$, characterizes an open-loop NE iff</p> <p>$$ \\begin{equation}     J(\\mathbf{u}^*_i|\\mathbf{x}_0,\\mathbf{u}^*_{-i}) \\leq \\inf_{\\mathbf{u}_i \\in \\mathcal{U}_i(\\mathbf{x}_0, \\mathbf{u}^*_{-i})}\\left\\{ J(\\mathbf{u}^*_i|\\mathbf{x}_0,\\mathbf{u}_{-i}) \\right\\} \\end{equation} $$</p> <p>which is satisfied by the fixed-point of the best response mapping of each agent, defined as</p> <p>$$ \\begin{equation}     \\label{eq:best_response}     \\mathbf{u}^*_i = \\underset{ {\\mathbf{u}_i \\in \\mathcal{U}(\\mathbf{x}_0,\\mathbf{u}^*_{-i})} }{ \\text{argmin} } J_i(\\mathbf{u}_i|\\mathbf{x}_0, \\mathbf{u}^*_{-i}), \\quad \\forall i \\in \\mathcal{N} \\end{equation} $$</p> <p>Proposition 2 in [1] states that any solution of the canonical VI is a solution for $\\eqref{eq:best_response}$ when $\\mathcal{S} = \\mathcal{U}(\\mathbf{x}_0)$ and $F : \\mathbb{R}^{mTN} \\to \\mathbb{R}^{mTN}$, defined as</p> <p>$$ \\begin{equation}     F(\\mathbf{u}) = \\text{col}(\\mathbf{G}^\\top_i \\bar{\\mathbf{Q}}_i)_{i \\in \\mathcal{N}} (\\text{row}(\\mathbf{G}_i)_{i \\in \\mathcal{N}}\\mathbf{u} + \\mathbf{H} \\mathbf{x}_0) +     \\text{blkdiag}(\\mathbf{I}_T \\otimes \\mathbf{R}_i)_{i \\in \\mathcal{N}} \\mathbf{u} \\end{equation} $$</p> <p>where, for all $i \\in \\mathcal{N}$, $\\bar{\\mathbf{Q}}_i = \\text{blkdiag}(\\mathbf{I}_{T-1} \\otimes \\mathbf{Q}_i, \\mathbf{P}_i)$, $\\mathbf{G}_i = \\mathbf{e}^\\top_{1,T} \\otimes \\text{col}(\\mathbf{A}^t_i \\mathbf{B}_i)_{t=0}^{T-1} + \\mathbf{I}_T \\otimes \\mathbf{B}_i$ and $\\mathbf{H} = \\text{col}(\\mathbf{A}^t)_{t = 1}^T$. Matrix $\\mathbf{P}_i$ results from the open-loop NE feedback synthesis as discussed in [Equation 6] [1].</p>"},{"location":"examples/linear-quadratic-game-nb/#references","title":"References\u00b6","text":"<p>[1] Benenati, E., &amp; Grammatico, S. (2024). Linear-Quadratic Dynamic Games as Receding-Horizon Variational Inequalities. arXiv preprint arXiv:2408.15703.</p>"},{"location":"examples/lipschitz/","title":"Prologue: approximating the Lipschitz constant","text":"In\u00a0[\u00a0]: Copied! <pre>def approximate_L(samples, box_size):\n    result = 0\n    for s in range(samples):\n        X, Y = np.random.uniform(-box_size/2, box_size/2, (2, n))\n        L = np.linalg.norm(F(X) - F(Y)) / np.linalg.norm(X - Y)\n        result = L if L &gt; result else result\n    return result\n</pre> def approximate_L(samples, box_size):     result = 0     for s in range(samples):         X, Y = np.random.uniform(-box_size/2, box_size/2, (2, n))         L = np.linalg.norm(F(X) - F(Y)) / np.linalg.norm(X - Y)         result = L if L &gt; result else result     return result In\u00a0[\u00a0]: Copied! <pre>samples = 10000\nbox_sizes = [10**n for n in range(13)]\n\nLs = [approximate_L(samples, box_size) for box_size in box_sizes]\n\n# Checking the values for the approximation\nplt.plot(Ls)\nplt.grid(alpha=0.2)\nplt.xlabel(\"n\")\nplt.ylabel(\"$L$ approximation\")\n\nplt.show()\n</pre> samples = 10000 box_sizes = [10**n for n in range(13)]  Ls = [approximate_L(samples, box_size) for box_size in box_sizes]  # Checking the values for the approximation plt.plot(Ls) plt.grid(alpha=0.2) plt.xlabel(\"n\") plt.ylabel(\"$L$ approximation\")  plt.show()"},{"location":"examples/lipschitz/#prologue-approximating-the-lipschitz-constant","title":"Prologue: approximating the Lipschitz constant\u00b6","text":"<p>This time, calculating the Lipschitz constant is a bit more convoluted. We can approximate it, leveraging the fact that $$ L \\in \\sup_{\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n} \\frac{\\|\\mathbf{F}(\\mathbf{x}) - \\mathbf{F}(\\mathbf{y})\\|}{\\| \\mathbf{x} - \\mathbf{y} \\| } \\geq \\tilde{L} := \\max_{\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n} \\frac{\\|\\mathbf{F}(\\mathbf{x}) - \\mathbf{F}(\\mathbf{y})\\|}{\\| \\mathbf{x} - \\mathbf{y} \\| } $$</p>"},{"location":"examples/logistic-regression-nb/","title":"Sparse logistic regression","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom monviso import VI\n\nnp.random.seed(2024)\n\nN, M = 500, 200\n\nA = np.random.normal(size=(M, N))\nb = np.random.choice([-1, 1], size=M)\ngamma = 0.005 * np.linalg.norm(A.T @ b, np.inf)\n</pre> import numpy as np import cvxpy as cp import matplotlib.pyplot as plt  from monviso import VI  np.random.seed(2024)  N, M = 500, 200  A = np.random.normal(size=(M, N)) b = np.random.choice([-1, 1], size=M) gamma = 0.005 * np.linalg.norm(A.T @ b, np.inf) <p>From the definition, the $\\mathbf{F}(\\cdot)$ and $g(\\cdot)$ are defined as</p> In\u00a0[2]: Copied! <pre>F = lambda x: -np.sum(\n    (A.T * np.tile(b, (N, 1))) * np.exp(-b * (A @ x)) / (1 + np.exp(-b * (A @ x))),\n    axis=1,\n)\ng = lambda x: gamma * cp.norm(x, 1)\nL = 1.5\n</pre> F = lambda x: -np.sum(     (A.T * np.tile(b, (N, 1))) * np.exp(-b * (A @ x)) / (1 + np.exp(-b * (A @ x))),     axis=1, ) g = lambda x: gamma * cp.norm(x, 1) L = 1.5 <p>Let's instantiate the VI and create an initial point</p> In\u00a0[3]: Copied! <pre>slr = VI(F, g=g)\nx = np.random.rand(N)\n</pre> slr = VI(F, g=g) x = np.random.rand(N) <p>As examples, let's use the Hybrid Golden Ratio Algorithm I (), the Golden ratio algorithm (), and the adaptive golden ratio algorithm ()</p> In\u00a0[30]: Copied! <pre>GOLDEN_RATIO = 0.5*(1 + np.sqrt(5))\nmax_iter = 200\n\nresiduals_hgraal = np.zeros(max_iter)\nresiduals_graal = np.zeros(max_iter)\nresiduals_agraal = np.zeros(max_iter)\n\nxk_hgraal, x1k_hgraal, yk_hgraal, s1k_hgraal, tk_hgraal, ck_hgraal = \\\n    np.copy(x), np.random.rand(N), np.copy(x), GOLDEN_RATIO/(2*L), 1, 1\nxk_graal, yk_graal = np.copy(x), np.copy(x)\nxk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = \\\n    np.copy(x), np.random.rand(N), np.copy(x), GOLDEN_RATIO/(2*L), 1\n\nfor k in range(max_iter):\n    # Hybrid Golden Ratio Algorithm I\n    xk1_hgraal, yk1_hgraal, sk_hgraal, tk1_hgraal, ck1_hgraal = \\\n        slr.hgraal_1(xk_hgraal, x1k_hgraal, yk_hgraal, s1k_hgraal, tk_hgraal, ck_hgraal)\n    \n    residuals_hgraal[k] = np.linalg.norm(xk1_hgraal - xk_hgraal)\n    \n    xk_hgraal, x1k_hgraal, yk_hgraal, s1k_hgraal, tk_hgraal, ck_hgraal = \\\n        xk1_hgraal, xk_hgraal, yk1_hgraal, sk_hgraal, tk1_hgraal, ck1_hgraal\n\n    # Golden ratio algorithm\n    xk1_graal, yk1_graal = slr.graal(xk_graal, yk_graal, step_size=GOLDEN_RATIO/(2*L))\n    residuals_graal[k] = np.linalg.norm(xk1_graal - xk_graal)\n    xk_graal, yk_graal = xk1_graal, yk1_graal\n    \n    # Adaptive golden ratio algorithm\n    xk1_agraal, yk1_agraal, sk_agraal, tk1_agraal = slr.agraal(xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal)\n    residuals_agraal[k] = np.linalg.norm(xk1_agraal - xk_agraal)\n    xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = xk1_agraal, xk_agraal, yk1_agraal, sk_agraal, tk1_agraal\n</pre> GOLDEN_RATIO = 0.5*(1 + np.sqrt(5)) max_iter = 200  residuals_hgraal = np.zeros(max_iter) residuals_graal = np.zeros(max_iter) residuals_agraal = np.zeros(max_iter)  xk_hgraal, x1k_hgraal, yk_hgraal, s1k_hgraal, tk_hgraal, ck_hgraal = \\     np.copy(x), np.random.rand(N), np.copy(x), GOLDEN_RATIO/(2*L), 1, 1 xk_graal, yk_graal = np.copy(x), np.copy(x) xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = \\     np.copy(x), np.random.rand(N), np.copy(x), GOLDEN_RATIO/(2*L), 1  for k in range(max_iter):     # Hybrid Golden Ratio Algorithm I     xk1_hgraal, yk1_hgraal, sk_hgraal, tk1_hgraal, ck1_hgraal = \\         slr.hgraal_1(xk_hgraal, x1k_hgraal, yk_hgraal, s1k_hgraal, tk_hgraal, ck_hgraal)          residuals_hgraal[k] = np.linalg.norm(xk1_hgraal - xk_hgraal)          xk_hgraal, x1k_hgraal, yk_hgraal, s1k_hgraal, tk_hgraal, ck_hgraal = \\         xk1_hgraal, xk_hgraal, yk1_hgraal, sk_hgraal, tk1_hgraal, ck1_hgraal      # Golden ratio algorithm     xk1_graal, yk1_graal = slr.graal(xk_graal, yk_graal, step_size=GOLDEN_RATIO/(2*L))     residuals_graal[k] = np.linalg.norm(xk1_graal - xk_graal)     xk_graal, yk_graal = xk1_graal, yk1_graal          # Adaptive golden ratio algorithm     xk1_agraal, yk1_agraal, sk_agraal, tk1_agraal = slr.agraal(xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal)     residuals_agraal[k] = np.linalg.norm(xk1_agraal - xk_agraal)     xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = xk1_agraal, xk_agraal, yk1_agraal, sk_agraal, tk1_agraal In\u00a0[35]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(residuals_hgraal, label=\"Hybrid Golden Ratio Algorithm I\")\nax.plot(residuals_graal, label=\"Golden ratio algorithm\")\nax.plot(residuals_agraal, label=\"Adaptive golden ratio algorithm\")\nplt.yscale('log')\n\nax.grid(True, alpha=0.2)\nax.set_xlabel(\"Iterations ($k$)\")\nax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\")\nplt.legend()\nplt.show()\n</pre> fig, ax = plt.subplots() ax.plot(residuals_hgraal, label=\"Hybrid Golden Ratio Algorithm I\") ax.plot(residuals_graal, label=\"Golden ratio algorithm\") ax.plot(residuals_agraal, label=\"Adaptive golden ratio algorithm\") plt.yscale('log')  ax.grid(True, alpha=0.2) ax.set_xlabel(\"Iterations ($k$)\") ax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\") plt.legend() plt.show()"},{"location":"examples/logistic-regression-nb/#sparse-logistic-regression","title":"Sparse logistic regression\u00b6","text":"<p>Consider a dataset of $M$ rows and $N$ columns, so that $\\mathbf{A} = \\text{col}(\\mathbf{a}^\\top_i)_{i =1}^M \\in \\mathbb{R}^{M \\times N}$ is the dataset matrix, and $\\mathbf{a}_i \\in \\mathbb{R}^{N}$ is the $i$-th features vector for the $i$-th dataset row. Moreover, let $\\mathbf{b} \\in \\mathbb{R}^M$ be the target vector, so that $b_i \\in \\{-1,1\\}$ is the (binary) ground truth for the $i$-th data entry. The sparse logistic regression consists of finding the weight vector $\\mathbf{x} \\in \\mathbb{R}^N$ that minimizes the following loss function [1]</p> <p>$$ \\begin{align}     \\label{eq:regression}         f(\\mathbf{x}) := \\sum_{i = 1}^M \\log\\left(1 + \\frac{1}{\\exp(b_i \\mathbf{a}^\\top_i \\mathbf{x})} \\right) + \\gamma \\|\\mathbf{x}\\|_1         \\\\ \\nonumber         = \\underbrace{\\mathbf{1}^\\top_M \\log(1 + \\exp(-\\mathbf{b} \\odot \\mathbf{A} \\mathbf{x}))}_{=:s(\\mathbf{x})} + \\underbrace{\\gamma \\|\\mathbf{x}\\|_1}_{=:g(\\mathbf{x})}  \\end{align} $$</p> <p>where $\\gamma \\in \\mathbb{R}_{&gt; 0}$ is the $\\ell_1$-regulation strength. The gradient for $s(\\cdot)$, $\\nabla s_\\mathbf{x}(\\mathbf{x})$, is calculated as</p> <p>$$ \\begin{equation}     F(\\mathbf{x}) = \\nabla s_\\mathbf{x}(\\mathbf{x}) = -\\frac{\\mathbf{A}^\\top \\odot (\\mathbf{1}_N \\otimes \\mathbf{b}^\\top) \\odot \\exp(-\\mathbf{b} \\odot \\mathbf{A} \\mathbf{x})}{1 + \\exp(-\\mathbf{b} \\odot \\mathbf{A} \\mathbf{x})} \\mathbf{1}_M \\end{equation} $$</p> <p>The problem of finding the minimizer for $\\eqref{eq:regression}$ can be cast as a canonical VI, with $F(\\mathbf{x}) := \\nabla s_\\mathbf{x}(\\mathbf{x})$. Let's start by defining the train matrix, target vector, and regularization strength.</p>"},{"location":"examples/logistic-regression-nb/#references","title":"References\u00b6","text":"<p>[1] Mishchenko, K. (2023). Regularized Newton method with global convergence. SIAM Journal on Optimization, 33(3), 1440-1462.</p>"},{"location":"examples/markov-decision-process-nb/","title":"Markov Decision Process","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom monviso import VI\n\nnp.random.seed(2024)\n\n# Number of states and actions, discount factor\nnum_X, num_A = 20, 10\ngamma = 0.8\n\n# Transition probabilities and reward\nP = np.random.rand(num_X, num_A, num_X)\nP /= P.sum(2, keepdims=True)\nR = np.random.rand(num_X, num_X)\n</pre> import numpy as np import cvxpy as cp import matplotlib.pyplot as plt  from monviso import VI  np.random.seed(2024)  # Number of states and actions, discount factor num_X, num_A = 20, 10 gamma = 0.8  # Transition probabilities and reward P = np.random.rand(num_X, num_A, num_X) P /= P.sum(2, keepdims=True) R = np.random.rand(num_X, num_X) <p>Let's define the Bellman operator (as fixed point), the VI mapping, and the initial solution</p> In\u00a0[2]: Copied! <pre>T = lambda v: np.einsum(\"ijk,ik -&gt; ij\", P, R + gamma * v[None, :]).max(1)\nF = lambda x: x - T(x)\n\nmdp = VI(F)\nx = np.random.rand(num_X)\n</pre> T = lambda v: np.einsum(\"ijk,ik -&gt; ij\", P, R + gamma * v[None, :]).max(1) F = lambda x: x - T(x)  mdp = VI(F) x = np.random.rand(num_X) <p>As examples, we can use the forward-reflected-backward (<code>frb</code>), the projected reflected gradient (<code>prg</code>), and the extra anchored gradient (<code>eag</code>).</p> In\u00a0[3]: Copied! <pre>max_iter = 300\nstep_size = 0.05\n\nresiduals_frb = np.zeros(max_iter)\nresiduals_prg = np.zeros(max_iter)\nresiduals_eag = np.zeros(max_iter)\n\n# Copy the same initial solution for the three methods\nxk_frb, x1k_frb = np.copy(x), np.copy(x)\nxk_prg, x1k_prg = np.copy(x), np.copy(x)\nxk_eag, x0_eag = np.copy(x), np.copy(x)\n\nfor k in range(max_iter):\n    # Forward-reflected-backward\n    xk1_frb = mdp.frb(xk_frb, x1k_frb, step_size)\n    residuals_frb[k] = np.linalg.norm(xk1_frb - xk_frb)\n    xk_frb, x1k_frb = xk1_frb, xk_frb\n\n    # Projected reflected gradient\n    xk1_prg = mdp.prg(xk_prg, x1k_prg, step_size)\n    residuals_prg[k] = np.linalg.norm(xk1_prg - xk_prg)\n    xk_prg = xk1_prg\n\n    # Extra anchored gradient\n    xk1_eag = mdp.eag(xk_eag, x0_eag, k, step_size)\n    residuals_eag[k] = np.linalg.norm(xk1_eag - xk_eag)\n    xk_eag = xk1_eag\n</pre> max_iter = 300 step_size = 0.05  residuals_frb = np.zeros(max_iter) residuals_prg = np.zeros(max_iter) residuals_eag = np.zeros(max_iter)  # Copy the same initial solution for the three methods xk_frb, x1k_frb = np.copy(x), np.copy(x) xk_prg, x1k_prg = np.copy(x), np.copy(x) xk_eag, x0_eag = np.copy(x), np.copy(x)  for k in range(max_iter):     # Forward-reflected-backward     xk1_frb = mdp.frb(xk_frb, x1k_frb, step_size)     residuals_frb[k] = np.linalg.norm(xk1_frb - xk_frb)     xk_frb, x1k_frb = xk1_frb, xk_frb      # Projected reflected gradient     xk1_prg = mdp.prg(xk_prg, x1k_prg, step_size)     residuals_prg[k] = np.linalg.norm(xk1_prg - xk_prg)     xk_prg = xk1_prg      # Extra anchored gradient     xk1_eag = mdp.eag(xk_eag, x0_eag, k, step_size)     residuals_eag[k] = np.linalg.norm(xk1_eag - xk_eag)     xk_eag = xk1_eag <p>The API docs have some detail on the convention for naming iterative steps. Let's check out the residuals for each method.</p> In\u00a0[4]: Copied! <pre>fig, ax = plt.subplots()\n\nax.plot(residuals_frb, label=\"Forward reflected backward\")\nax.plot(residuals_prg, label=\"Forward-backward-forward\")\nax.plot(residuals_eag, label=\"Extra anchored gradient\")\n\nax.grid(True, alpha=0.2)\nax.set_xlabel(\"Iterations ($k$)\")\nax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\")\nplt.legend()\nplt.show()\n</pre> fig, ax = plt.subplots()  ax.plot(residuals_frb, label=\"Forward reflected backward\") ax.plot(residuals_prg, label=\"Forward-backward-forward\") ax.plot(residuals_eag, label=\"Extra anchored gradient\")  ax.grid(True, alpha=0.2) ax.set_xlabel(\"Iterations ($k$)\") ax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\") plt.legend() plt.show()"},{"location":"examples/markov-decision-process-nb/#markov-decision-process","title":"Markov Decision Process\u00b6","text":"<p>A stationary discrete Markov Decision Process (MDP) is characterized by the tuple $(\\mathcal{X},\\mathcal{A},\\mathbb{P},r,\\gamma)$, where</p> <ol> <li>$\\mathcal{X}$ is the (finite countable) set of states;</li> <li>$\\mathcal{A}$ is the (finite countable) set of actions;</li> <li>$P : \\mathcal{X} \\times \\mathcal{A} \\times \\mathcal{X} \\to [0,1]$ is the transition probability function, such that $P(x,a,x^+)$ is the probability of ending up in state $x^+ \\in \\mathcal{S}$ from state $x \\in \\mathcal{X}$ when taking action $a \\in \\mathcal{A}$;</li> <li>$r : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ is the reward function, so that $r(x,x^+)$ returns the reward for transitioning from state $x \\in \\mathcal{X}$ to state $x^+ \\in \\mathcal{X}$;</li> <li>$\\gamma \\in \\mathbb{R}_{&gt; 0}$ is a discount factor [1].</li> </ol> <p>The aim is to find a policy, i.e., a function $\\pi : \\mathcal{S} \\to \\mathcal{A}$, returning the best action for any given state. A solution concept for MDP is the value function, $v^{\\pi} : \\mathcal{S} \\to \\mathbb{R}$, defined as $$ \\begin{equation}     \\label{eq:bellman}     v^{\\pi}(x) = \\overbrace{\\sum_{x^+ \\in \\mathcal{X}} P(x,\\pi(x),x^+) \\left( r(x,x^+) + \\gamma v(x^+) \\right)}^{=:\\mathsf{T}(v^{\\pi})} \\end{equation} $$</p> <p>returning the \"goodness\" of policy $\\pi$. The expression in $\\eqref{eq:bellman}$ is known as Bellman equation, and can be expressed as an operator of $v^{\\pi}$, i.e., $\\mathsf{T}[v^\\pi(s)] =: \\mathsf{T}(v^{\\pi})$. It can be shown that the value function yielded by the optimal policy, $v^*$, results from the fixed-point problem $v^* = \\mathsf{T}(v^*)$. Therefore, the latter can be formulated as a canonical VI, with $F = \\mathsf{I} - \\mathsf{T}$.</p> <p>Let's start by defining the problem data</p>"},{"location":"examples/markov-decision-process-nb/#references","title":"References\u00b6","text":"<p>[1] Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.</p>"},{"location":"examples/quickstart-nb/","title":"Quickstart","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport cvxpy as cp\n\nimport matplotlib.pyplot as plt\n\nfrom monviso import VI\n\nnp.random.seed(2024)\n\n# Create the problem data\nn, m = 30, 40\nA = np.random.uniform(0, 1, size=(m, n))\nb = np.random.uniform(45, 50, size=(m,))\n\n# Define S\ny = cp.Variable(n)\nS = [A @ y &gt;= b, y &gt;= 1]\n</pre> import numpy as np import cvxpy as cp  import matplotlib.pyplot as plt  from monviso import VI  np.random.seed(2024)  # Create the problem data n, m = 30, 40 A = np.random.uniform(0, 1, size=(m, n)) b = np.random.uniform(45, 50, size=(m,))  # Define S y = cp.Variable(n) S = [A @ y &gt;= b, y &gt;= 1] <p>Also, let's define $\\mathbf{F}(\\mathbf{x}) = \\mathbf{H} \\mathbf{x}$, for some $\\mathbf{H} \\succ \\mathbf{0}$, as the VI vector mapping, and $g(\\mathbf{x}) = \\|\\mathbf{x}\\|_1$ as the scalar mapping.</p> In\u00a0[2]: Copied! <pre># Create a positive definite H\nH = np.random.uniform(2, 10, size=(n, n))\nH = H @ H.T\n\n# Define F, g\nF = lambda x: H @ x\ng = lambda x: cp.norm(x, 1)\n</pre> # Create a positive definite H H = np.random.uniform(2, 10, size=(n, n)) H = H @ H.T  # Define F, g F = lambda x: H @ x g = lambda x: cp.norm(x, 1) <p>It is straightforward to verify that $\\mathbf{F}(\\cdot)$ is strongly monotone with $\\mu = \\lambda_{\\min}(\\mathbf{H})$ and Lipschitz with $L = \\|\\mathbf{H}\\|_2$.</p> In\u00a0[3]: Copied! <pre>mu = np.linalg.eigvals(H).min()\nL = np.linalg.norm(H, 2)\n</pre> mu = np.linalg.eigvals(H).min() L = np.linalg.norm(H, 2) <p>Strong monotonicity allows us to use the proximal gradient algorithm to solve such a VI, with a step size of $\\chi = 2\\mu / L^2$.</p> In\u00a0[4]: Copied! <pre>step_size = 2 * mu / L**2\n</pre> step_size = 2 * mu / L**2 <p>Let's declare this VI in <code>monviso</code> first.</p> In\u00a0[5]: Copied! <pre>vi = VI(y, F, g, S)\n</pre> vi = VI(y, F, g, S) <p>Then, let's create an initial solution.</p> In\u00a0[6]: Copied! <pre>xk = np.random.uniform(0, 1, n)\n</pre> xk = np.random.uniform(0, 1, n) <p>In <code>monviso</code>, the proximal gradient method is called <code>pg</code>. Let's iterate over it and collect the distance between two consecutive iteration results in a <code>residual</code> vector.</p> In\u00a0[7]: Copied! <pre>max_iters = 10\nresiduals = np.zeros(max_iters)\nfor k in range(max_iters):\n    xk1 = vi.pg(xk, step_size)        \n    residuals[k] = np.linalg.norm(xk - xk1)\n    xk = xk1\n</pre> max_iters = 10 residuals = np.zeros(max_iters) for k in range(max_iters):     xk1 = vi.pg(xk, step_size)             residuals[k] = np.linalg.norm(xk - xk1)     xk = xk1 <p>The API docs have some detail on the convention for naming iterative steps. Let's take a look at the residuals</p> In\u00a0[8]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(residuals)\nax.grid(True, alpha=0.2)\nax.set_xlabel(\"Iterations ($k$)\")\nax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\")\nplt.show()\n</pre> fig, ax = plt.subplots() ax.plot(residuals) ax.grid(True, alpha=0.2) ax.set_xlabel(\"Iterations ($k$)\") ax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\") plt.show()"},{"location":"examples/quickstart-nb/#quickstart","title":"Quickstart\u00b6","text":"<p>As a first simple example, let's consider a vector set $\\mathcal{S} = \\{\\mathbf{y} \\in \\mathbb{R}^n : \\mathbf{A} \\mathbf{y} \\geq \\mathbf{b}, \\mathbf{y} \\geq \\mathbf{1} \\}$, for some $n,m \\in \\mathbb{R}$ and for some $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^n$.</p>"},{"location":"examples/skew-symmetric-nb/","title":"Skew-symmetric operator","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport scipy as sp\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom monviso import VI\n\nnp.random.rand(2024)\n\nM, N = 20, 10\nmake_positive_semidefinite = lambda X: X * X.T\n\nBs = [make_positive_semidefinite(np.random.rand(N, N)) for _ in range(M)]\nA = sp.linalg.block_diag(*[np.tril(B) - np.triu(B) for B in Bs])\n</pre> import numpy as np import scipy as sp import cvxpy as cp import matplotlib.pyplot as plt  from monviso import VI  np.random.rand(2024)  M, N = 20, 10 make_positive_semidefinite = lambda X: X * X.T  Bs = [make_positive_semidefinite(np.random.rand(N, N)) for _ in range(M)] A = sp.linalg.block_diag(*[np.tril(B) - np.triu(B) for B in Bs]) <p>Then, let's define $\\mathbf{F}(\\cdot)$ with it's Lipschitz constant $L$</p> In\u00a0[2]: Copied! <pre>F = lambda x: A @ x\nL = np.linalg.norm(A, 2)\n</pre> F = lambda x: A @ x L = np.linalg.norm(A, 2) <p>Finally, let's create the VI problem and an initial solution</p> In\u00a0[3]: Copied! <pre>sso = VI(F)\nx = np.random.rand(N * M)\n</pre> sso = VI(F) x = np.random.rand(N * M) <p>The vector map $\\mathbf{F}(\\cdot)$ is merely monotone; we can use and compare three different algorithms: fast optimistic gradient descent-ascent (<code>fogda</code>), golden ratio (<code>graal</code>), adaptive golden ratio (<code>agraal</code>)</p> In\u00a0[5]: Copied! <pre>GOLDEN_RATIO = 0.5 * (1 + np.sqrt(5))\n\nmax_iter = 200\n\nresiduals_fogda = np.zeros(max_iter)\nresiduals_graal = np.zeros(max_iter)\nresiduals_agraal = np.zeros(max_iter)\n\n# Copy the same initial solution for the three methods\nxk_fogda, x1k_fogda, y1k_fogda = np.copy(x), np.copy(x), np.copy(x)\nxk_graal, yk_graal = np.copy(x), np.copy(x)\nxk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = \\\n    np.copy(x), np.random.rand(N * M), np.copy(x), GOLDEN_RATIO/(2*L), 1\n\nfor k in range(max_iter):\n    # Fast Optimistic Gradient Descent Ascent \n    xk1_fogda, yk_fogda = sso.fogda(xk_fogda, x1k_fogda, y1k_fogda, k, step_size=1/(4*L))\n    residuals_fogda[k] = np.linalg.norm(xk1_fogda - xk_fogda)\n    xk_fogda, x1k_fogda, y1k_fogda = xk1_fogda, xk_fogda, yk_fogda\n\n    # Golden ratio algorithm\n    xk1_graal, yk1_graal = sso.graal(xk_graal, yk_graal, step_size=GOLDEN_RATIO/(2*L))\n    residuals_graal[k] = np.linalg.norm(xk1_graal - xk_graal)\n    xk_graal, yk_graal = xk1_graal, yk1_graal\n\n    # Adaptive golden ratio algorithm\n    xk1_agraal, yk1_agraal, sk_agraal, tk1_agraal = sso.agraal(xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal)\n    residuals_agraal[k] = np.linalg.norm(xk1_agraal - xk_agraal)\n    xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = xk1_agraal, xk_agraal, yk1_agraal, sk_agraal, tk1_agraal\n</pre> GOLDEN_RATIO = 0.5 * (1 + np.sqrt(5))  max_iter = 200  residuals_fogda = np.zeros(max_iter) residuals_graal = np.zeros(max_iter) residuals_agraal = np.zeros(max_iter)  # Copy the same initial solution for the three methods xk_fogda, x1k_fogda, y1k_fogda = np.copy(x), np.copy(x), np.copy(x) xk_graal, yk_graal = np.copy(x), np.copy(x) xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = \\     np.copy(x), np.random.rand(N * M), np.copy(x), GOLDEN_RATIO/(2*L), 1  for k in range(max_iter):     # Fast Optimistic Gradient Descent Ascent      xk1_fogda, yk_fogda = sso.fogda(xk_fogda, x1k_fogda, y1k_fogda, k, step_size=1/(4*L))     residuals_fogda[k] = np.linalg.norm(xk1_fogda - xk_fogda)     xk_fogda, x1k_fogda, y1k_fogda = xk1_fogda, xk_fogda, yk_fogda      # Golden ratio algorithm     xk1_graal, yk1_graal = sso.graal(xk_graal, yk_graal, step_size=GOLDEN_RATIO/(2*L))     residuals_graal[k] = np.linalg.norm(xk1_graal - xk_graal)     xk_graal, yk_graal = xk1_graal, yk1_graal      # Adaptive golden ratio algorithm     xk1_agraal, yk1_agraal, sk_agraal, tk1_agraal = sso.agraal(xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal)     residuals_agraal[k] = np.linalg.norm(xk1_agraal - xk_agraal)     xk_agraal, x1k_agraal, yk_agraal, s1k_agraal, tk_agraal = xk1_agraal, xk_agraal, yk1_agraal, sk_agraal, tk1_agraal <p>The API docs have some detail on the convention for naming iterative steps. Let's check out the residuals for each method.</p> In\u00a0[6]: Copied! <pre>fig, ax = plt.subplots()\n\nax.plot(residuals_fogda, label=\"Fast Optimistic Gradient Descent Ascent \")\nax.plot(residuals_graal, label=\"Golden ratio algorithm\")\nax.plot(residuals_agraal, label=\"Adaptive golden ratio algorithm\")\n\nax.grid(True, alpha=0.2)\nax.set_xlabel(\"Iterations ($k$)\")\nax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\")\nplt.legend()\nplt.show()\n</pre> fig, ax = plt.subplots()  ax.plot(residuals_fogda, label=\"Fast Optimistic Gradient Descent Ascent \") ax.plot(residuals_graal, label=\"Golden ratio algorithm\") ax.plot(residuals_agraal, label=\"Adaptive golden ratio algorithm\")  ax.grid(True, alpha=0.2) ax.set_xlabel(\"Iterations ($k$)\") ax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\") plt.legend() plt.show()"},{"location":"examples/skew-symmetric-nb/#skew-symmetric-operator","title":"Skew-symmetric operator\u00b6","text":"<p>A simple example of monotone operator that is not (even locally) strongly monotone is the skewed-symmetric operator [1], $F : \\mathbb{R}^{MN} \\to \\mathbb{R}^{MN}$, which is described as follows</p> <p>$$ \\begin{equation}     F(\\mathbf{x}) = \\begin{bmatrix} \\mathbf{A}_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\mathbf{A}_M \\end{bmatrix} \\mathbf{x} \\end{equation} $$</p> <p>for a given $M \\in \\mathbb{N}$, where $\\mathbf{A}_i = \\text{tril}(\\mathbf{B}_i) - \\text{triu}(\\mathbf{B}_i)$, for some arbitrary $0 \\preceq \\mathbf{B}_i \\in \\mathbb{R}^{N \\times N}$, for all $i = 1, \\dots, M$. Let's implement it by first creating the problem data</p>"},{"location":"examples/skew-symmetric-nb/#references","title":"References\u00b6","text":"<p>[1] Bauschke, H. H., &amp; Combettes, P. L. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.</p>"},{"location":"examples/zero-sum-game-nb/","title":"Two Players Zero-Sum Game","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\nfrom monviso import VI\n\nnp.random.seed(2024)\n\nn1, n2 = 50, 50\n\n# Simplex constraints' set\ny = cp.Variable(n1 + n2)\nS = [cp.sum(y[:n1]) == 1, cp.sum(y[n1:]) == 1]\n</pre> import numpy as np import cvxpy as cp import matplotlib.pyplot as plt  from monviso import VI  np.random.seed(2024)  n1, n2 = 50, 50  # Simplex constraints' set y = cp.Variable(n1 + n2) S = [cp.sum(y[:n1]) == 1, cp.sum(y[n1:]) == 1] <p>The players try to solve the following problem</p> <p>$$ \\begin{equation}     \\min_{\\mathbf{x}_1 \\in \\Delta_1} \\max_{\\mathbf{x}_2 \\in \\Delta_2} \\Phi(\\mathbf{x}_1, \\mathbf{x}_2) \\end{equation} $$</p> <p>whose (Nash) equilibrium solution is achieved for $\\mathbf{x}^*$ satisfying the following</p> <p>$$ \\begin{equation}     \\label{eq:saddle}     \\Phi(\\mathbf{x}^*_1, \\mathbf{x}_2) \\leq \\Phi(\\mathbf{x}^*_1, \\mathbf{x}^*_2) \\leq \\Phi(\\mathbf{x}_1, \\mathbf{x}^*_2), \\quad \\forall \\mathbf{x} \\in \\Delta_1 \\times \\Delta_2 \\end{equation} $$</p> <p>For the sake of simplicity, we consider $\\Phi(\\mathbf{x}_1, \\mathbf{x}_2) := \\mathbf{x}^\\top_1 \\mathbf{H} \\mathbf{x}_2$, for some $\\mathbf{H} \\in \\mathbb{R}^{n_1 \\times n_2}$. Doing so, the equilibrium condition in the previous equation can be written as a VI, with the mapping $\\mathbf{F} : \\mathbb{R}^{n_1 + n_2} \\to \\mathbb{R}^{n_1 + n_2}$ defined as</p> <p>$$ \\begin{equation}     \\mathbf{F}(\\mathbf{x}) = \\begin{bmatrix} \\mathbf{H} \\mathbf{x}_2 \\\\ -\\mathbf{H}^\\top \\mathbf{x}_1 \\end{bmatrix} = \\underbrace{\\begin{bmatrix} &amp; \\mathbf{H} \\\\ -\\mathbf{H}^\\top &amp; \\end{bmatrix} \\mathbf{x}}_{=:\\mathbf{G}} \\end{equation} $$</p> <p>Let's start from creating $\\mathbf{G}$ and the vector mapping $\\mathbf{F}(\\cdot)$:</p> In\u00a0[2]: Copied! <pre>H = np.random.rand(n1, n2)\nG = np.block([\n    [np.zeros((n1, n2)), H], \n    [-H.T, np.zeros((n1, n2))]\n])\n\nF = lambda x: G @ x\n</pre> H = np.random.rand(n1, n2) G = np.block([     [np.zeros((n1, n2)), H],      [-H.T, np.zeros((n1, n2))] ])  F = lambda x: G @ x <p>$\\mathbf{F}(\\cdot)$ is Lipschitz, being linear, with constant $L = \\|\\mathbf{G}\\|_2$.</p> In\u00a0[3]: Copied! <pre>L = np.linalg.norm(G, 2)\n</pre> L = np.linalg.norm(G, 2) <p>Let's create the VI associated with the game and an initial solution</p> In\u00a0[4]: Copied! <pre>tpzsg = VI(F, y=y, S=S)\nx = np.random.rand(n1 + n2)\n</pre> tpzsg = VI(F, y=y, S=S) x = np.random.rand(n1 + n2) <p>In this case, $\\mathbf{F}(\\cdot)$ is merely monotone, so we don't have guarantees for proximal gradient to converge (although it might still be used). However, there are a lot of alternative algorithms for merely monotone VIs. Let's compare three of them: extragradient (<code>eg</code>), Forward-backward-forward (<code>fbf</code>), and Popov's method (<code>popov</code>).</p> In\u00a0[5]: Copied! <pre>max_iter = 1000\n\nresiduals_eg = np.zeros(max_iter)\nresiduals_fbf = np.zeros(max_iter)\nresiduals_popov = np.zeros(max_iter)\n\n# Copy the same initial solution for the three methods\nxk_eg = np.copy(x)\nxk_fbf = np.copy(x)\n\nxk_popov = np.copy(x)\nyk_popov = np.copy(xk_popov)\n\nfor k in range(max_iter):\n    # Extragradient\n    xk1_eg = tpzsg.eg(xk_eg, 1 / L)\n    residuals_eg[k] = np.linalg.norm(xk_eg - xk1_eg)\n    xk_eg = xk1_eg\n    \n    # Forward-backward-forward\n    xk1_fbf = tpzsg.fbf(xk_fbf, 1 / L)\n    residuals_fbf[k] = np.linalg.norm(xk_fbf - xk1_fbf)\n    xk_fbf = xk1_fbf\n\n    # Popov's method\n    xk1_popov, yk1_popov = tpzsg.popov(xk_popov, yk_popov, 1 / (2*L))\n    residuals_popov[k] = np.linalg.norm(xk_popov - xk1_popov)\n    xk_popov = xk1_popov\n    yk_popov = yk1_popov\n</pre> max_iter = 1000  residuals_eg = np.zeros(max_iter) residuals_fbf = np.zeros(max_iter) residuals_popov = np.zeros(max_iter)  # Copy the same initial solution for the three methods xk_eg = np.copy(x) xk_fbf = np.copy(x)  xk_popov = np.copy(x) yk_popov = np.copy(xk_popov)  for k in range(max_iter):     # Extragradient     xk1_eg = tpzsg.eg(xk_eg, 1 / L)     residuals_eg[k] = np.linalg.norm(xk_eg - xk1_eg)     xk_eg = xk1_eg          # Forward-backward-forward     xk1_fbf = tpzsg.fbf(xk_fbf, 1 / L)     residuals_fbf[k] = np.linalg.norm(xk_fbf - xk1_fbf)     xk_fbf = xk1_fbf      # Popov's method     xk1_popov, yk1_popov = tpzsg.popov(xk_popov, yk_popov, 1 / (2*L))     residuals_popov[k] = np.linalg.norm(xk_popov - xk1_popov)     xk_popov = xk1_popov     yk_popov = yk1_popov <p>The API docs have some detail on the convention for naming iterative steps. Let's check out the residuals for each method.</p> In\u00a0[6]: Copied! <pre>fig, ax = plt.subplots()\n\nax.plot(residuals_eg, label=\"Extragradient\")\nax.plot(residuals_fbf, label=\"Forward-backward-forward\")\nax.plot(residuals_popov, label=\"Popov's method\")\n\nax.grid(True, alpha=0.2)\nax.set_xlabel(\"Iterations ($k$)\")\nax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\")\nplt.legend()\nplt.show()\n</pre> fig, ax = plt.subplots()  ax.plot(residuals_eg, label=\"Extragradient\") ax.plot(residuals_fbf, label=\"Forward-backward-forward\") ax.plot(residuals_popov, label=\"Popov's method\")  ax.grid(True, alpha=0.2) ax.set_xlabel(\"Iterations ($k$)\") ax.set_ylabel(r\"$\\|\\mathbf{x}_k - \\mathbf{x}_{k+1}\\|$\") plt.legend() plt.show()"},{"location":"examples/zero-sum-game-nb/#two-players-zero-sum-game","title":"Two Players Zero-Sum Game\u00b6","text":"<p>Many example of non-cooperative behavior between two adversarial agents can be modelled through zero-sum games [1]. Let us consider vectors $\\mathbf{x}_i \\in \\Delta_i$ as the decision variable of the $i$-th player, with $i \\in \\{1,2\\}$, where $\\Delta_i \\subset \\mathbb{R}^{n_i}$ is the simplex constraints set defined as $\\Delta_i := \\{\\mathbf{x} \\in \\mathbb{R}^{n_i} : \\mathbf{1}^\\top \\mathbf{x} = 1\\}$, for all $i \\in \\{1,2\\}$. Let $\\mathbf{x} := \\text{col}(\\mathbf{x}_i)_{i = 1}^2$. The set of the combined decisions, for the two agents, is thus $\\mathcal{S} = \\Delta_1 \\times \\Delta_2$. Let's create it.</p>"},{"location":"examples/zero-sum-game-nb/#references","title":"References\u00b6","text":"<p>[1] Lemke, C. E., &amp; Howson, Jr, J. T. (1964). Equilibrium points of bimatrix games. Journal of the Society for industrial and Applied Mathematics, 12(2), 413-423.</p>"}]}